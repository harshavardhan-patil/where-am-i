{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-21 09:36:31.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: G:\\Work\\DS\\where-am-i\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import geopy as gp\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from src.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos = pd.read_parquet(RAW_DATA_DIR / 'train.parquet').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Nominatim API \n",
    "geolocator = Nominatim(user_agent=\"GetLoc\")\n",
    "geolocator.reverse((19.134120, -155.505545)).raw['address']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric - Geodesic Distance\n",
    " Specifically, for each query image, we compute the Geodesic\n",
    " distance between GPS coordinates predicted by our model and the respective ground truth. We\n",
    " calculate how many of them (in %) fall within the distance thresholds (1km, 25km, 200km, 750km,\n",
    " and 2500km) and report average performance of model over three runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion - Cosine Similarity\n",
    "\n",
    "#### Precision levels\n",
    "|precision  | (maximum X axis error, in km)    |\n",
    "|--|--| \n",
    "1  | ± 2500\n",
    "2  | ± 630\n",
    "3   |± 78\n",
    "4   |± 20\n",
    "5   |± 2.4\n",
    "6   |± 0.61\n",
    "7   |± 0.076\n",
    "8   |± 0.019\n",
    "9   |± 0.0024\n",
    "10  |± 0.00060\n",
    "11  |± 0.000074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geohash: 8e\n",
      "Decoded Coordinates: (20.0, -152.0)\n",
      "380.110257016121\n"
     ]
    }
   ],
   "source": [
    "import pygeohash as pgh\n",
    "\n",
    "# Example coordinates\n",
    "latitude = 19.134120\n",
    "longitude = -155.505545\n",
    "\n",
    "# Encode coordinates into geohash\n",
    "geohash = pgh.encode(latitude, longitude, precision=2)  # Precision determines geohash length\n",
    "print(f\"Geohash: {geohash}\")\n",
    "\n",
    "# Decode geohash back to coordinates\n",
    "decoded_coords = pgh.decode(geohash)\n",
    "print(f\"Decoded Coordinates: {decoded_coords}\")\n",
    "\n",
    "from geopy import distance\n",
    "from pygeohash.distances import geohash_approximate_distance\n",
    "\n",
    "og = (latitude, longitude)\n",
    "decoded = decoded_coords\n",
    "\n",
    "print(distance.distance(og, decoded).km)\n",
    "#print(geohash_approximate_distance('8e3wd2hw8', '8e3wh209f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygeohash as pgh\n",
    "\n",
    "\n",
    "def get_hashes(r: pd.Series):\n",
    "    hash = pgh.encode(r['latitude'], r['longitude'], precision = 9)\n",
    "    r['coarse'] = hash[:4]\n",
    "    r['medium'] = hash[:6]\n",
    "    r['fine'] = hash\n",
    "    return r\n",
    "\n",
    "hashed_annos = annos.apply(get_hashes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashed_annos.to_parquet(INTERIM_DATA_DIR / 'hashed_annos.parquet', index = False)\n",
    "hashed_annos = pd.read_parquet(INTERIM_DATA_DIR / 'hashed_annos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_coarse = hashed_annos.loc[:, ['coarse']]['coarse'].unique()\n",
    "num_medium = hashed_annos.loc[:, ['medium']]['medium'].unique()\n",
    "num_fine = hashed_annos.loc[:, ['fine']]['fine'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['8e3w', '8e3w7q', '8e3w7qcfmc'],\n",
       "       ['8e3w', '8e3w5w', '8e3w5w9yqb']], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [319277780027588, 472478470643498]\n",
    "hashed_annos.set_index(keys='id').loc[l, [ 'coarse', 'medium', 'fine']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206098"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_imgs = 0\n",
    "for root, dirs, files in os.walk(INTERIM_DATA_DIR / 'train'):\n",
    "    for file in files:\n",
    "        num_imgs+=1\n",
    "\n",
    "num_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pre_train = pd.read_csv(INTERIM_DATA_DIR / 'train/pre_train.csv').drop(columns=['Unnamed: 0', 'Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1206098/1206100 [00:00<00:00, 1718026.12it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'pre_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(INTERIM_DATA_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m     12\u001b[0m             dic_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mid\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'pre_train.csv'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "df_pre_train = pd.read_csv(INTERIM_DATA_DIR / 'train/pre_train.csv')\n",
    "ids = set(df_pre_train.loc[:, 'id'].values.tolist())\n",
    "dic_ids = []\n",
    "for root, dirs, files in os.walk(INTERIM_DATA_DIR / 'train'):\n",
    "    for file in tqdm(files):\n",
    "        id = int(file.split('.jpg')[0])\n",
    "        if id not in ids:\n",
    "            dic_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6862/6862 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "df_test = pd.read_csv(INTERIM_DATA_DIR / 'test.csv')\n",
    "ids = set(df_test.loc[:, 'id'].values.tolist())\n",
    "dic_ids = []\n",
    "for root, dirs, files in os.walk(INTERIM_DATA_DIR / 'val'):\n",
    "    for file in tqdm(files):\n",
    "        id = int(file.split('.jpg')[0])\n",
    "        if id in ids:\n",
    "            dic_ids.append(id)\n",
    "        else:\n",
    "            os.remove(os.path.join(root, file))\n",
    "\n",
    "\n",
    "df_val = df_test.set_index(keys='id').loc[dic_ids,].reset_index()\n",
    "df_val.to_csv(INTERIM_DATA_DIR / 'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(INTERIM_DATA_DIR / 'val/04'):\n",
    "    for file in tqdm(files):\n",
    "        id = int(file.split('.jpg')[0])\n",
    "        if id in ids:\n",
    "            dic_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_train = pd.concat([df_pre_train, hashed_annos.set_index(keys='id').loc[dic_ids, [ 'coarse', 'medium', 'fine']].reset_index()])\n",
    "df_pre_train.to_csv(INTERIM_DATA_DIR / 'train/pre_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_pre_train.drop(columns=[col for col in df_pre_train.columns.to_list() if col not in ('id', 'coarse', 'medium', 'fine')])\n",
    "df_train['coarse_i'], class_mapping = pd.factorize(df_train['coarse'])\n",
    "df_train['medium_i'], class_mapping = pd.factorize(df_train['medium'])\n",
    "df_train['fine_i'], class_mapping = pd.factorize(df_train['fine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=['coarse', 'medium', 'fine']).to_csv(INTERIM_DATA_DIR / 'train/train.csv', index = False)\n",
    "#df_train = pd.read_csv(INTERIM_DATA_DIR / 'train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre_train = pd.read_csv(INTERIM_DATA_DIR / 'pre_train.csv')\n",
    "len(df_pre_train['coarse'].str[:3].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not using since pretrained ViT has inbuilt logic\n",
    "class ViTPreProcessor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTPreProcessor, self).__init__()\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size = (KERNEL_SIZE, KERNEL_SIZE), stride = KERNEL_SIZE) \n",
    "        self.patch_embed = nn.Linear(CHANNELS * KERNEL_SIZE ** 2, EMBED_DIM) \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, EMBED_DIM)) # 1, 1, EMBED_DIM\n",
    "        self.postional_embeds = nn.Parameter(torch.zeros(1, NUM_PATCHES + 1, EMBED_DIM))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0) #B\n",
    "\n",
    "        patches = self.unfold(x) # B, C * KERNEL_SIZE^2, NUM_PATCHES per img \n",
    "        patches = patches.transpose(1, 2)\n",
    "        patch_embeddings = self.patch_embed(patches) # B, NUM_PATCHES per img, EMBED_DIM\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # B, 1, EMBED_DIM\n",
    "        cls_patches = torch.cat((cls_token, patch_embeddings), dim=1) # B, NUM_PATCHES per img + 1, EMBED_DIM\n",
    "        \n",
    "        return cls_patches + self.postional_embeds # B, NUM_PATCHES per img + 1, EMBED_DIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "where-am-i-s7vJJwrF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
