{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopy as gp\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from src.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 83742/1206098 [00:06<01:33, 11972.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 1 with 83866 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83866/83866 [06:38<00:00, 210.49it/s]t/s]\n",
      " 14%|█▍        | 166408/1206098 [06:51<01:26, 11972.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 2 with 82844 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82844/82844 [07:35<00:00, 181.79it/s]it/s]\n",
      " 21%|██        | 250786/1206098 [14:45<03:08, 5064.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 3 with 84223 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84223/84223 [07:27<00:00, 188.04it/s]t/s]\n",
      " 28%|██▊       | 335454/1206098 [22:35<03:32, 4094.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 4 with 84596 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84596/84596 [07:43<00:00, 182.36it/s]t/s]\n",
      " 35%|███▍      | 418702/1206098 [30:34<02:17, 5743.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 5 with 83200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83200/83200 [07:36<00:00, 182.42it/s]t/s]\n",
      " 42%|████▏     | 502449/1206098 [38:19<01:09, 10097.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 6 with 83865 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83865/83865 [07:19<00:00, 190.88it/s]it/s]\n",
      " 49%|████▊     | 585931/1206098 [45:50<01:19, 7826.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 7 with 83922 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83922/83922 [07:49<00:00, 178.90it/s]t/s]\n",
      " 56%|█████▌    | 669919/1206098 [53:54<01:17, 6903.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 8 with 84027 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84027/84027 [07:42<00:00, 181.61it/s]t/s]\n",
      " 62%|██████▏   | 753174/1206098 [1:01:50<01:12, 6290.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 9 with 83117 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83117/83117 [07:30<00:00, 184.54it/s]7it/s]\n",
      " 69%|██████▉   | 837104/1206098 [1:09:33<00:50, 7302.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 10 with 83721 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83721/83721 [08:01<00:00, 173.72it/s]2it/s]\n",
      " 76%|███████▋  | 921369/1206098 [1:17:47<00:38, 7318.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 11 with 84320 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84320/84320 [07:38<00:00, 183.86it/s]6it/s]\n",
      " 83%|████████▎ | 1004910/1206098 [1:25:37<00:26, 7673.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 12 with 83473 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83473/83473 [08:01<00:00, 173.51it/s]70it/s]\n",
      " 90%|█████████ | 1088022/1206098 [1:33:52<00:20, 5704.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 13 with 83362 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83362/83362 [07:52<00:00, 176.34it/s]58it/s]\n",
      " 97%|█████████▋| 1172470/1206098 [1:42:03<00:06, 5068.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing chunk 14 with 84577 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84577/84577 [07:32<00:00, 186.76it/s]76it/s]\n",
      "100%|██████████| 1206098/1206098 [1:49:41<00:00, 183.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing final chunk 15 with 32985 images...\n",
      "All chunks created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import math\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Define paths\n",
    "IMAGE_FOLDER = INTERIM_DATA_DIR / 'train'  # Replace with the folder containing images\n",
    "CSV_FILE = INTERIM_DATA_DIR / 'train.csv'  # CSV file with the image IDs\n",
    "OUTPUT_DIR = PROCESSED_DATA_DIR / 'train'  # Folder where compressed chunks will be saved\n",
    "CHUNK_SIZE_GB = 4  # Target size of each chunk in GB\n",
    "IMAGE_EXTENSION = '.jpg'  # Image file format\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Read image IDs from the CSV file\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "image_ids = df['id'].astype(str).tolist()  # Ensure IDs are strings\n",
    "\n",
    "# Function to calculate size of images\n",
    "def get_image_size(image_path):\n",
    "    return os.path.getsize(image_path) / (1024 ** 3)  # Convert bytes to GB\n",
    "\n",
    "# Create chunks\n",
    "current_chunk = 1\n",
    "current_chunk_size = 0\n",
    "current_images = []\n",
    "\n",
    "for img_id in tqdm(image_ids):\n",
    "    img_path = os.path.join(IMAGE_FOLDER, f\"{img_id}{IMAGE_EXTENSION}\")\n",
    "    if os.path.exists(img_path):\n",
    "        img_size = get_image_size(img_path)\n",
    "        # Check if adding this image exceeds the chunk size\n",
    "        if current_chunk_size + img_size > CHUNK_SIZE_GB:\n",
    "            # Compress the current chunk\n",
    "            chunk_name = os.path.join(OUTPUT_DIR, f\"train_{current_chunk}.zip\")\n",
    "            print(f\"Compressing chunk {current_chunk} with {len(current_images)} images...\")\n",
    "            with ZipFile(chunk_name, 'w') as zipf:\n",
    "                for image in tqdm(current_images):\n",
    "                    zipf.write(image, os.path.basename(image))  # Add image to the zip\n",
    "            # Reset for the next chunk\n",
    "            current_chunk += 1\n",
    "            current_chunk_size = 0\n",
    "            current_images = []\n",
    "        \n",
    "        # Add image to the current chunk\n",
    "        current_images.append(img_path)\n",
    "        current_chunk_size += img_size\n",
    "    else:\n",
    "        print(f\"Warning: {img_path} not found.\")\n",
    "\n",
    "# Compress the remaining images in the last chunk\n",
    "if current_images:\n",
    "    chunk_name = os.path.join(OUTPUT_DIR, f\"train_{current_chunk}.zip\")\n",
    "    print(f\"Compressing final chunk {current_chunk} with {len(current_images)} images...\")\n",
    "    with ZipFile(chunk_name, 'w') as zipf:\n",
    "        for image in current_images:\n",
    "            zipf.write(image, os.path.basename(image))\n",
    "\n",
    "print(\"All chunks created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting G:\\Work\\DS\\where-am-i\\data\\processed\\train\\train_15.zip...\n",
      "All images extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Define paths\n",
    "DRIVE_FOLDER = PROCESSED_DATA_DIR / 'train'  # Folder where chunks are stored\n",
    "EXTRACTION_FOLDER = PROCESSED_DATA_DIR / 't'  # Folder to extract images\n",
    "\n",
    "os.makedirs(EXTRACTION_FOLDER, exist_ok=True)\n",
    "\n",
    "# List chunks\n",
    "chunks = [f for f in os.listdir(DRIVE_FOLDER) if f == 'train_15.zip']\n",
    "\n",
    "# Extract all chunks\n",
    "for chunk in chunks:\n",
    "    chunk_path = os.path.join(DRIVE_FOLDER, chunk)\n",
    "    print(f\"Extracting {chunk_path}...\")\n",
    "    shutil.unpack_archive(chunk_path, EXTRACTION_FOLDER)\n",
    "\n",
    "print(\"All images extracted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "where-am-i-s7vJJwrF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
