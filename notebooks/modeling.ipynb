{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKdekHQOFzm",
        "outputId": "60a9861c-4f73-4d70-e790-9abb935685d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import geopy as gp\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/Colab/where-am-i/data'  # Folder where chunks are stored\n",
        "EXTRACTION_FOLDER = '/content/train'  # Folder to extract images\n",
        "\n",
        "os.makedirs(EXTRACTION_FOLDER, exist_ok=True)\n",
        "\n",
        "# List chunks\n",
        "chunks = [f for f in os.listdir(DRIVE_FOLDER) if f == 'train_15.zip']\n",
        "\n",
        "# Extract all chunks\n",
        "for chunk in chunks:\n",
        "    chunk_path = os.path.join(DRIVE_FOLDER, chunk)\n",
        "    print(f\"Extracting {chunk_path}...\")\n",
        "    shutil.unpack_archive(chunk_path, EXTRACTION_FOLDER)\n",
        "\n",
        "print(\"All images extracted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2pCaI0wPhJ-",
        "outputId": "f3408a84-bc55-4754-a7ec-d058e0314e68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_15.zip...\n",
            "All images extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVD9thsoOFzp",
        "outputId": "7911d0ed-0274-4e45-b630-b5b50d156a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32985/32985 [00:00<00:00, 255836.79it/s]\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab/where-am-i/train.csv')\n",
        "INTERIM_DATA_DIR = '/content/'\n",
        "\n",
        "ids = set(df_train.loc[:, 'id'].values.tolist())\n",
        "dic_ids = []\n",
        "for root, dirs, files in os.walk('/content/train'):\n",
        "    for file in tqdm(files):\n",
        "        id = int(file.split('.jpg')[0])\n",
        "        if id in ids:\n",
        "            dic_ids.append(id)\n",
        "df_train = df_train.set_index(keys='id').loc[dic_ids,]\n",
        "trainset = df_train.reset_index().iloc[:int(len(df_train) * 0.9)]\n",
        "valset = df_train.reset_index().iloc[int(len(df_train) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVImageDataset(Dataset):\n",
        "    def __init__(self, annotations_df, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = annotations_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0]) + '.jpg')\n",
        "        image = decode_image(img_path).float() / 255.0\n",
        "        label = torch.tensor((self.img_labels.iloc[idx, 1], self.img_labels.iloc[idx, 2], self.img_labels.iloc[idx, 3]))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        image = image.clamp(0, 1)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "DaiToKzTSet-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4zX5Uy2OFzq",
        "outputId": "0ea7429d-b0ee-4b87-81ab-9a2a0a1fe25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "KERNEL_SIZE = 16 #16x16 patch\n",
        "CHANNELS = 3 #rgb\n",
        "RESIZE = 224\n",
        "EMBED_DIM = CHANNELS * KERNEL_SIZE ** 2\n",
        "NUM_PATCHES = ((RESIZE + 0 - KERNEL_SIZE)//KERNEL_SIZE + 1) ** 2\n",
        "COARSE = int(df_train['coarse_i'].values.max()) + 1\n",
        "MEDIUM = int(df_train['medium_i'].values.max()) + 1\n",
        "FINE = int(df_train['fine_i'].values.max()) + 1\n",
        "MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "\n",
        "#Using values the ViT was trained on\n",
        "processor = ViTImageProcessor.from_pretrained(MODEL_NAME, do_rescale = False, return_tensors = 'pt')\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "      v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "      #v2.RandomHorizontalFlip(0.4),\n",
        "      #v2.RandomVerticalFlip(0.1),\n",
        "      #v2.RandomApply(transforms=[v2.RandomRotation(degrees=(0, 90))], p=0.5),\n",
        "      #v2.RandomApply(transforms=[v2.ColorJitter(brightness=.3, hue=.1)], p=0.3),\n",
        "      #v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9))], p=0.3),\n",
        "      normalize\n",
        " ])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVImageDataset(annotations_df = trainset, img_dir=INTERIM_DATA_DIR + 'train', transform=train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataset = OSVImageDataset(annotations_df = valset, img_dir = INTERIM_DATA_DIR + 'train', transform=test_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D3r9q2WJOFzq"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "\n",
        "class GeoLocator(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(GeoLocator, self).__init__()\n",
        "\n",
        "        self.backbone = ViTModel.from_pretrained(MODEL_NAME,)\n",
        "\n",
        "        self.coarse_layer = nn.Linear(self.backbone.config.hidden_size, COARSE)\n",
        "        self.medium_layer = nn.Linear(self.backbone.config.hidden_size, MEDIUM)\n",
        "        self.fine_layer = nn.Linear(self.backbone.config.hidden_size, FINE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.backbone(x).last_hidden_state\n",
        "        outputs = outputs[:, 0, :] #for classification only need CLS token\n",
        "\n",
        "        coarse_output = self.coarse_layer(outputs)\n",
        "        medium_output = self.medium_layer(outputs)\n",
        "        fine_output = self.fine_layer(outputs)\n",
        "\n",
        "        return coarse_output, medium_output, fine_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OfB03_vIYNux"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sHK76DaOOFzr"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "import time\n",
        "\n",
        "model = GeoLocator().to(device=device)\n",
        "#freezing backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#optimizer for custom layers only\n",
        "#AdamW was unstable and resulted in memory overflow. Using SGD\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': model.coarse_layer.parameters()},\n",
        "    {'params': model.medium_layer.parameters()},\n",
        "    {'params': model.fine_layer.parameters()}\n",
        "], lr = 0.0001)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "\n",
        "        with autocast(device_type=device.__str__()):\n",
        "            coarse_output, medium_output, fine_output = model(images)\n",
        "\n",
        "            coarse_loss = criterion(coarse_output, labels[:, 0])\n",
        "            medium_loss = criterion(medium_output, labels[:, 1])\n",
        "            fine_loss = criterion(fine_output, labels[:, 2])\n",
        "\n",
        "            loss = 0.6 * coarse_loss + 0.8 * medium_loss + fine_loss\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del images, labels, coarse_output, medium_output, fine_output, coarse_loss, medium_loss, fine_loss\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        del loss\n",
        "        gc.collect()\n",
        "\n",
        "        torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "-SUIA4w5Ytfy",
        "outputId": "f6795c3a-9730-4685-b3ee-78d4a0b6cf7a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [13:27<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loss' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e1ddaeb1ee25>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary())"
      ],
      "metadata": {
        "id": "Bn1YXzqgXMdl",
        "outputId": "3decb2d7-a68b-43f6-bb18-4d8b5ded0ccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  10078 MiB |  10704 MiB |  17119 MiB |   7041 MiB |\n",
            "|       from large pool |  10077 MiB |  10702 MiB |  17038 MiB |   6961 MiB |\n",
            "|       from small pool |      1 MiB |      3 MiB |     81 MiB |     80 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  10078 MiB |  10704 MiB |  17119 MiB |   7041 MiB |\n",
            "|       from large pool |  10077 MiB |  10702 MiB |  17038 MiB |   6961 MiB |\n",
            "|       from small pool |      1 MiB |      3 MiB |     81 MiB |     80 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  10075 MiB |  10701 MiB |  17082 MiB |   7007 MiB |\n",
            "|       from large pool |  10074 MiB |  10699 MiB |  17001 MiB |   6927 MiB |\n",
            "|       from small pool |      1 MiB |      3 MiB |     81 MiB |     80 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  12102 MiB |  12102 MiB |  14620 MiB |   2518 MiB |\n",
            "|       from large pool |  12098 MiB |  12098 MiB |  14616 MiB |   2518 MiB |\n",
            "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  48860 KiB |   1176 MiB |   3093 MiB |   3045 MiB |\n",
            "|       from large pool |  47952 KiB |   1174 MiB |   3010 MiB |   2963 MiB |\n",
            "|       from small pool |    908 KiB |      2 MiB |     83 MiB |     82 MiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     220    |     236    |     753    |     533    |\n",
            "|       from large pool |      86    |      91    |     379    |     293    |\n",
            "|       from small pool |     134    |     148    |     374    |     240    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     220    |     236    |     753    |     533    |\n",
            "|       from large pool |      86    |      91    |     379    |     293    |\n",
            "|       from small pool |     134    |     148    |     374    |     240    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      33    |      35    |      39    |       6    |\n",
            "|       from large pool |      31    |      33    |      37    |       6    |\n",
            "|       from small pool |       2    |       2    |       2    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      27    |      31    |     372    |     345    |\n",
            "|       from large pool |      20    |      26    |     249    |     229    |\n",
            "|       from small pool |       7    |       9    |     123    |     116    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bTkEZPK5OFzr"
      },
      "outputs": [],
      "source": [
        "#from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"/content/drive/MyDrive/Colab/where-am-i/geonn_test.pt\"\n",
        "torch.save(model.state_dict(), str(nn_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIhKWyMrOFzs",
        "outputId": "e65a1815-7861-4651-e819-af2e67935c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "fine_true = np.array([])\n",
        "fine_pred = np.empty(shape=(0, FINE))\n",
        "coarse_true = np.array([])\n",
        "coarse_pred = np.empty(shape=(0, COARSE))\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        coarse_output, _, fine_output = model(images)\n",
        "\n",
        "        coarse_true = np.concatenate((coarse_true, labels[:,0].cpu()), axis=0)\n",
        "        coarse_pred = np.concatenate((coarse_pred, coarse_output.cpu()), axis=0)\n",
        "        fine_true = np.concatenate((fine_true, labels[:, 2].cpu()), axis=0)\n",
        "        fine_pred = np.concatenate((fine_pred, fine_output.cpu()), axis=0)\n",
        "\n",
        "        del images, labels, coarse_output, fine_output\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "JtB3O731OFzs",
        "outputId": "8528e403-94ab-45c2-d00a-314e40f88868"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a38ef295d5f7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_accuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mtop_k_accuracy_score\u001b[0;34m(y_true, y_score, k, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \"\"\"\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1088\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "print(f'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}')\n",
        "print(f'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBi4K_aiOFzt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}