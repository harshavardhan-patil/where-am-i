{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-12-25 18:36:35.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: G:\\Work\\DS\\where-am-i\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import geopy as gp\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from src.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, INTERIM_DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_train.drop(columns=['coarse', 'medium', 'fine']).to_csv(INTERIM_DATA_DIR / 'train/train.csv', index = False)\n",
        "df_train = pd.read_csv(INTERIM_DATA_DIR / 'train.csv')\n",
        "df_train = pd.merge(df_train, pd.read_parquet(INTERIM_DATA_DIR / 'hashed_annos.parquet').loc[:, ['id', 'latitude', 'longitude']], on='id')#.drop(columns=['coarse_i','medium_i','fine_i'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_train.to_csv(INTERIM_DATA_DIR / 'pos_train.csv', index=False)\n",
        "df_train = pd.read_csv(INTERIM_DATA_DIR / 'pos_train.csv').drop(columns=['coarse_i', 'medium_i', 'fine_i'])\n",
        "trainset = df_train.iloc[:int(len(df_train) * 0.9)]\n",
        "valset = df_train.iloc[int(len(df_train) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "class OSVImageDataset(Dataset):\n",
        "    def __init__(self, annotations_df, img_dir, transform=None):\n",
        "        self.img_labels = annotations_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.temp = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #todo: idx using image id?\n",
        "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0]) + '.jpg')\n",
        "        a = time.perf_counter()\n",
        "        image = decode_image(img_path).float() / 255.0\n",
        "        b = time.perf_counter()\n",
        "        self.temp.append(b-a)\n",
        "        label = torch.tensor([self.img_labels.iloc[idx, 1], self.img_labels.iloc[idx, 2]])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.clamp(0, 1)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "[0.5, 0.5, 0.5] [0.5, 0.5, 0.5]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "KERNEL_SIZE = 16 #16x16 patch\n",
        "CHANNELS = 3 #rgb\n",
        "RESIZE = 224\n",
        "EMBED_DIM = CHANNELS * KERNEL_SIZE ** 2\n",
        "NUM_PATCHES = ((RESIZE + 0 - KERNEL_SIZE)//KERNEL_SIZE + 1) ** 2\n",
        "MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "#Using values the ViT was trained on\n",
        "processor = ViTImageProcessor.from_pretrained(MODEL_NAME, do_rescale = False, return_tensors = 'pt')\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "      v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "      v2.RandomHorizontalFlip(0.4),\n",
        "      v2.RandomVerticalFlip(0.1),\n",
        "      v2.RandomApply(transforms=[v2.RandomRotation(degrees=(0, 90))], p=0.5),\n",
        "      v2.RandomApply(transforms=[v2.ColorJitter(brightness=.3, hue=.1)], p=0.3),\n",
        "      v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9))], p=0.3),\n",
        "      normalize\n",
        " ])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "'''train_dataset = OSVImageDataset(annotations_df = trainset, img_dir=INTERIM_DATA_DIR / 'train', transform=train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = OSVImageDataset(annotations_df = valset, img_dir = INTERIM_DATA_DIR / 'train', transform=test_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)'''\n",
        "print(image_mean, image_std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "\n",
        "class GeoLocator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocator, self).__init__()\n",
        "        \n",
        "        self.backbone = ViTModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        self.layer1 = nn.Linear(self.backbone.config.hidden_size, self.backbone.config.hidden_size)\n",
        "        self.norm1 = nn.LayerNorm(self.backbone.config.hidden_size)\n",
        "        self.dropout1 = nn.Dropout(p=0.05)  \n",
        "        \n",
        "        self.layer2 = nn.Linear(self.backbone.config.hidden_size, 512)  # 512: embedding size of location encoder\n",
        "        self.norm2 = nn.LayerNorm(512)\n",
        "        self.dropout2 = nn.Dropout(p=0.05)\n",
        "        \n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract last hidden state from the ViT backbone\n",
        "        x1 = self.backbone(x).last_hidden_state\n",
        "        x1 = x1[:, 0, :]  # Use CLS token only\n",
        "\n",
        "        # First layer with normalization and dropout\n",
        "        a1 = func.leaky_relu(self.norm1(self.layer1(x1)))\n",
        "        a1 = self.dropout1(a1)\n",
        "        \n",
        "        # Second layer with normalization and dropout\n",
        "        a2 = func.leaky_relu(self.norm2(self.layer2(a1)))\n",
        "        a2 = self.dropout2(a2)\n",
        "        \n",
        "        # Output layer\n",
        "        output = self.layer3(a2)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "\n",
        "model = GeoLocator().to(device=device)\n",
        "#freezing backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#optimizer for custom layers only\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.layer1.parameters()},\n",
        "    {'params': model.layer2.parameters()},\n",
        "    {'params': model.layer3.parameters()}\n",
        "], lr = 0.001)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CosineEmbeddingLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(device=device)\n",
        "        labels = gps_encoder(labels.float().to(device=device))\n",
        "        with autocast(device_type=device.__str__()):\n",
        "            output = model(images)\n",
        "            ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "            loss = criterion(output, labels, ones)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del images, labels, ones\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"geonn_t.pt\"\n",
        "torch.save(model.state_dict(), str(nn_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        coarse_output, _, fine_output = model(images)\n",
        "\n",
        "        coarse_true = np.concatenate((coarse_true, labels[:,0].cpu()), axis=0)\n",
        "        coarse_pred = np.concatenate((coarse_pred, coarse_output.cpu()), axis=0)\n",
        "        fine_true = np.concatenate((fine_true, labels[:, 2].cpu()), axis=0)\n",
        "        fine_pred = np.concatenate((fine_pred, fine_output.cpu()), axis=0)\n",
        "        \n",
        "        del images, labels, coarse_output, fine_output\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "print(f'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}')\n",
        "print(f'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18.75"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((1.2e6/32000) * 30)/60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T4 Perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.126436781609195"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#512 stable\n",
        "(1.2e6 / ((29000) / 124))/3600  * 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.765625"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((1.2e6/512) * 3)/3600 * 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14.0625"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#T4\n",
        "((1.2e6/512) * 3)/3600 * 5 * 1.44"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A100 Perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0919540229885059"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#1024 stable - underutilized\n",
        "(1.2e6 / ((29000) / 19))/3600 * 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.0345052083333335"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "((1.2e6/2048) * 2.5)/3600 * 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17.781575520833336"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#A100\n",
        "((1.2e6/2048) * 2.5)/3600 * 5 * 8.74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "((3.2e4/64) * 2)/60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.2291666666666667"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(1.2e6 / ((32000) / 59))/3600 * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Batch Perf check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total time for 64: 0.9269523000111803\n",
            "transform time for 64: 0.9166492000222206\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    images = images.to(device=device)\n",
        "    #labels = gps_encoder(labels.float().to(device=device))\n",
        "    c1 = time.perf_counter()\n",
        "    '''with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    del loss\n",
        "    gc.collect()'''\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c1 - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5918922002310865"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(train_dataset.temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = df_train.rename(columns={'coarse_i':'coarse', 'medium_i':'medium', 'fine_i':'fine'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_train.iloc[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_top_k_similar_embeddings(query_embedding, candidate_embeddings, k):\n",
        "    \"\"\"\n",
        "    Find the top k embeddings from the candidates based on cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        query_embedding (torch.Tensor): The embedding to compare against, shape (1, 512).\n",
        "        candidate_embeddings (torch.Tensor): Candidate embeddings, shape (N, 512).\n",
        "        k (int): Number of top embeddings to return.\n",
        "\n",
        "    Returns:\n",
        "        top_k_indices (list): Indices of the top k most similar embeddings.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), candidate_embeddings.cpu().numpy())[0]\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Sort in descending order\n",
        "    return top_k_indices\n",
        "\n",
        "def predict_locations(images, labels, dataloader, GeoLocator, gps_encoder, dataframe, k=5):\n",
        "    \"\"\"\n",
        "    Predict top k latitude and longitude/geohashes for given images.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images.\n",
        "        labels (torch.Tensor): Ground truth latitudes and longitudes.\n",
        "        dataloader (DataLoader): Dataloader for images and labels.\n",
        "        GeoLocator (torch.nn.Module): Trained model for predicting location embeddings.\n",
        "        gps_encoder (function): Function that encodes (lat, lon) into a 512D vector.\n",
        "        dataframe (pd.DataFrame): Dataframe with columns ['id', 'latitude', 'longitude', 'coarse', 'medium', 'fine'].\n",
        "        k (int): Number of top similar locations to return.\n",
        "\n",
        "    Returns:\n",
        "        results (list): A list of dictionaries with final top k predictions.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Precompute embeddings for unique coarse, medium, and fine geohashes\n",
        "    unique_coarse = dataframe.drop_duplicates(subset=['coarse'])\n",
        "    unique_medium = dataframe.drop_duplicates(subset=['medium'])\n",
        "    unique_fine = dataframe.drop_duplicates(subset=['fine'])\n",
        "\n",
        "    coarse_coords = torch.tensor(unique_coarse[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    medium_coords = torch.tensor(unique_medium[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    fine_coords = torch.tensor(unique_fine[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "\n",
        "    unique_coarse['embedding'] = gps_encoder(coarse_coords)\n",
        "    unique_medium['embedding'] = gps_encoder(medium_coords)\n",
        "    unique_fine['embedding'] = gps_encoder(fine_coords)\n",
        "\n",
        "    coarse_embeddings = torch.stack(unique_coarse['embedding'].tolist())\n",
        "    medium_embeddings = torch.stack(unique_medium['embedding'].tolist())\n",
        "    fine_embeddings = torch.stack(unique_fine['embedding'].tolist())\n",
        "\n",
        "    for batch_images, batch_labels in dataloader:\n",
        "        batch_results = []\n",
        "\n",
        "        # Predict embeddings using GeoLocator\n",
        "        predicted_embeddings = GeoLocator(batch_images)\n",
        "\n",
        "        for pred_embedding in predicted_embeddings:\n",
        "            # Step 1: Find top k coarse geohashes\n",
        "            top_k_coarse_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), coarse_embeddings, k)\n",
        "            top_k_coarse_candidates = unique_coarse.iloc[top_k_coarse_indices]\n",
        "\n",
        "            # Step 2: Find top k medium geohashes\n",
        "            medium_subset = unique_medium[unique_medium['coarse'].isin(top_k_coarse_candidates['coarse'])]\n",
        "            medium_embeddings_subset = torch.stack(medium_subset['embedding'].tolist())\n",
        "            top_k_medium_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), medium_embeddings_subset, k)\n",
        "            top_k_medium_candidates = medium_subset.iloc[top_k_medium_indices]\n",
        "\n",
        "            # Step 3: Find top k fine geohashes\n",
        "            fine_subset = unique_fine[unique_fine['medium'].isin(top_k_medium_candidates['medium'])]\n",
        "            fine_embeddings_subset = torch.stack(fine_subset['embedding'].tolist())\n",
        "            top_k_fine_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), fine_embeddings_subset, k)\n",
        "            final_top_k = fine_subset.iloc[top_k_fine_indices]\n",
        "\n",
        "            # Retrieve final top k latitudes and longitudes\n",
        "            batch_results.append(final_top_k[['latitude', 'longitude', 'fine']])\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding to GPS Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_train.drop(columns=['id', 'coarse_i', 'medium_i', 'fine_i'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "coords = torch.tensor(df[[\"latitude\", \"longitude\"]].values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = gps_encoder(coords).cpu().numpy()\n",
        "\n",
        "embedding_df = pd.DataFrame(\n",
        "    embeddings, \n",
        "    columns=[f\"embedding_{i+1}\" for i in range(embeddings.shape[1])]\n",
        ")\n",
        "\n",
        "# Concatenate the original DataFrame with the embeddings DataFrame\n",
        "df = pd.concat([df, embedding_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_parquet(INTERIM_DATA_DIR / 'embeddings.parquet').drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingToGPSDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingToGPSDecoder, self).__init__()\n",
        "\n",
        "        # First layer: 512 (embedding size) -> 512\n",
        "        self.layer1 = nn.Linear(512, 512)\n",
        "        self.norm1 = nn.LayerNorm(512)\n",
        "\n",
        "        # Second layer: 512 -> 256\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.norm2 = nn.LayerNorm(256)\n",
        "\n",
        "        # Output layer: 256 -> 2 (latitude and longitude)\n",
        "        self.output_layer = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with normalization and dropout\n",
        "        x1 = F.leaky_relu(self.norm1(self.layer1(x)))\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        x2 = F.leaky_relu(self.norm2(self.layer2(x1)))\n",
        "\n",
        "        # Output layer for predicting latitude and longitude\n",
        "        gps_coordinates = self.output_layer(x2)\n",
        "\n",
        "        return gps_coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, annotations_df):\n",
        "        self.df = annotations_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
        "        label = torch.tensor([self.df.iloc[idx, 0], self.df.iloc[idx, 1]])\n",
        "        return embedding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GeodesicDistanceLoss(nn.Module):\n",
        "    def __init__(self, radius=6371):\n",
        "        super(GeodesicDistanceLoss, self).__init__()\n",
        "        self.radius = radius  # Earth's radius in kilometers\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Convert latitude and longitude from degrees to radians\n",
        "        pred_rad = torch.deg2rad(pred)\n",
        "        target_rad = torch.deg2rad(target)\n",
        "\n",
        "        # Split latitudes and longitudes\n",
        "        lat1, lon1 = pred_rad[:, 0], pred_rad[:, 1]\n",
        "        lat2, lon2 = target_rad[:, 0], target_rad[:, 1]\n",
        "\n",
        "        # Haversine formula\n",
        "        delta_lat = lat2 - lat1\n",
        "        delta_lon = lon2 - lon1\n",
        "        a = torch.sin(delta_lat / 2) ** 2 + \\\n",
        "            torch.cos(lat1) * torch.cos(lat2) * torch.sin(delta_lon / 2) ** 2\n",
        "        c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "\n",
        "        # Distance in kilometers\n",
        "        distance = self.radius * c\n",
        "        return distance.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 1536\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVEmbeddingsDataset(annotations_df = df)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/786 [00:00<?, ?it/s]C:\\Users\\Harshavardhan Patil\\AppData\\Local\\Temp\\ipykernel_31752\\4128849953.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
            " 15%|█▍        | 114/786 [01:57<11:33,  1.03s/it]"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reverse/reversenn.pt\", weights_only=True))\n",
        "model = model.to(device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "criterion = GeodesicDistanceLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for embeddings, labels in tqdm(train_dataloader):\n",
        "        embeddings = embeddings.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        \n",
        "        output = model(embeddings)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del output, embeddings, labels\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step(loss) \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        nn_path = f\"/content/drive/MyDrive/Colab/where-am-i/reversenn_v{epoch}.pt\"\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            #'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'scheduler_state_dict': scheduler.state_kjtdict()\n",
        "        }, str(nn_path))\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"reversenn.pt\"\n",
        "torch.save(model.state_dict(), MODELS_DIR / nn_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_parquet(INTERIM_DATA_DIR / 'embeddings.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.loc[:,['id','latitude','longitude']].equals(pd.concat([df_train['id'], df], axis=1).loc[:, ['id','latitude','longitude']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([df_train['id'], df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "\u001b[32m2024-12-23 12:45:20.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: G:\\Work\\DS\\where-am-i\u001b[0m\n",
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "from src.base.EmbeddingToGPSDecoder import EmbeddingToGPSDecoder\n",
        "from src.base.GeoLocator import GeoLocator\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import v2\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from src.base.GeoLocator import GeoLocator\n",
        "from src.base.EmbeddingToGPSDecoder import EmbeddingToGPSDecoder\n",
        "from geopy.geocoders import Nominatim\n",
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "\n",
        "gps_encoder = LocationEncoder()\n",
        "\n",
        "\n",
        "nn_path = MODELS_DIR / 'geonn/geoswin_v3.pt'\n",
        "decoder_path = MODELS_DIR / \"reverse/reversenn.pt\"\n",
        "\n",
        "model = GeoLocator()\n",
        "model.load_state_dict(torch.load(nn_path, weights_only=True)['model_state_dict'])\n",
        "gps_decoder = EmbeddingToGPSDecoder()\n",
        "gps_decoder.load_state_dict(torch.load(decoder_path, weights_only=True))\n",
        "model.eval()\n",
        "gps_decoder.eval()\n",
        "\n",
        "embeds = gps_encoder(torch.tensor([[46.1609733604378, -123.3200203758254]]))\n",
        "output = gps_decoder(embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "uploaded_file = \"106315642303467.jpg\"\n",
        "def image_to_tensor(uploaded_file):\n",
        "    # Open the image\n",
        "    image = Image.open(uploaded_file)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        v2.Resize((224, 224)),  \n",
        "        v2.ToTensor(),         \n",
        "        v2.Normalize(\n",
        "           mean=[0.0, 0.0, 0.0],  \n",
        "           std=[255.0, 255.0, 255.0]\n",
        "        ),\n",
        "        v2.Normalize(\n",
        "           mean=[0.485, 0.456, 0.406],\n",
        "           std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    tensor = transform(image)\n",
        "    return tensor   \n",
        "\n",
        "\n",
        "model.eval()\n",
        "gps_decoder.eval()\n",
        "image = image_to_tensor(uploaded_file).unsqueeze(0) #adding a bacth dimension for vit\n",
        "\n",
        "with torch.no_grad():\n",
        "    embedding = model(image)\n",
        "    output = gps_decoder(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{'county': 'Cowlitz County',\n",
        " 'state': 'Washington',\n",
        " 'ISO3166-2-lvl4': 'US-WA',\n",
        " 'country': 'United States',\n",
        " 'country_code': 'us'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'county': 'Loup County',\n",
              " 'state': 'Nebraska',\n",
              " 'ISO3166-2-lvl4': 'US-NE',\n",
              " 'postcode': '68879',\n",
              " 'country': 'United States',\n",
              " 'country_code': 'us'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "from src.config import MODELS_DIR, PROCESSED_DATA_DIR\n",
        "\n",
        "# initialize Nominatim API \n",
        "geolocator = Nominatim(user_agent=\"GetLoc\")\n",
        "geolocator.reverse(output).raw['address']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  41.3965, -111.4439]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45.75356838199693\n"
          ]
        }
      ],
      "source": [
        "from geopy import distance\n",
        "\n",
        "og = (41.659354389516, -111.86601253359)\n",
        "decoded = (41.3965, -111.4439)\n",
        "\n",
        "print(distance.distance(og, decoded).km)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "where-am-i-s7vJJwrF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
