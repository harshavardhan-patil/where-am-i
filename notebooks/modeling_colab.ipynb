{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install geoclip\n",
        "!pip install nvidia-dali-cuda120"
      ],
      "metadata": {
        "id": "2bJsRpIb1_r6",
        "outputId": "0c12e485-6658-477c-ad62-1e839e7d0635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoclip\n",
            "  Downloading geoclip-1.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from geoclip) (0.20.1+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from geoclip) (11.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from geoclip) (4.47.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoclip) (1.26.4)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy->geoclip) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->geoclip) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->geoclip) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->geoclip) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (2024.12.14)\n",
            "Downloading geoclip-1.2.0-py3-none-any.whl (40.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geoclip\n",
            "Successfully installed geoclip-1.2.0\n",
            "Collecting nvidia-dali-cuda120\n",
            "  Downloading nvidia_dali_cuda120-1.44.0.tar.gz (1.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: astunparse<=1.6.3,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.6.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.6.0)\n",
            "Collecting six<=1.16,>=1.16 (from nvidia-dali-cuda120)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: dm-tree<=0.1.8 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.1.8)\n",
            "Requirement already satisfied: packaging<=24.2 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (24.2)\n",
            "Collecting nvidia-nvimgcodec-cu12<0.4.0,>=0.3.0 (from nvidia-dali-cuda120)\n",
            "  Downloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl.metadata (761 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse<=1.6.3,>=1.6.0->nvidia-dali-cuda120) (0.45.1)\n",
            "Downloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: nvidia-dali-cuda120\n",
            "  Building wheel for nvidia-dali-cuda120 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-dali-cuda120: filename=nvidia_dali_cuda120-1.44.0-20402542-py3-none-manylinux2014_x86_64.whl size=323567102 sha256=400b7b8db95a87702994a8f0475a29f463598a9a240de2c192423acdb0461909\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0c/89/4bc8a1da0bf7abe83d8759855f7cf5a3fa72c50a87eb89764d\n",
            "Successfully built nvidia-dali-cuda120\n",
            "Installing collected packages: six, nvidia-nvimgcodec-cu12, nvidia-dali-cuda120\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "Successfully installed nvidia-dali-cuda120-1.44.0 nvidia-nvimgcodec-cu12-0.3.0.5 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              },
              "id": "947ed2d7e94f45f7a0c6e6dddac3c4c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5aoD10vio64o",
        "outputId": "c6b688c5-1b67-422c-b480-3456b7258537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import geopy as gp\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PkRC3vzno64p",
        "outputId": "eb14e285-18a9-4b8d-f7a9-485430139163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_15.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:38<00:00, 38.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images extracted successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/Colab/where-am-i/data'  # Folder where chunks are stored\n",
        "EXTRACTION_FOLDER = '/content/train'  # Folder to extract images\n",
        "\n",
        "os.makedirs(EXTRACTION_FOLDER, exist_ok=True)\n",
        "\n",
        "# List chunks\n",
        "#chunks = [f for f in os.listdir(DRIVE_FOLDER) if f.endswith('.zip')]\n",
        "chunks = [f for f in os.listdir(DRIVE_FOLDER) if f == 'train_15.zip']\n",
        "\n",
        "# Extract all chunks\n",
        "for chunk in tqdm(chunks):\n",
        "    chunk_path = os.path.join(DRIVE_FOLDER, chunk)\n",
        "    print(f\"Extracting {chunk_path}...\")\n",
        "    shutil.unpack_archive(chunk_path, EXTRACTION_FOLDER)\n",
        "\n",
        "print(\"All images extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train = pd.read_parquet('/content/drive/MyDrive/Colab/where-am-i/embeddings.parquet')"
      ],
      "metadata": {
        "id": "Hiuk6DbOvxG1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YuYALNDjo64p",
        "outputId": "8d00bbc6-5d70-436d-da28-de0a3895b0d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32985/32985 [00:00<00:00, 1048051.74it/s]\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab/where-am-i/pos_train.csv')\n",
        "INTERIM_DATA_DIR = '/content/'\n",
        "\n",
        "ids = set(df_train.loc[:, 'id'].values.tolist())\n",
        "dic_ids = []\n",
        "for root, dirs, files in os.walk('/content/train'):\n",
        "    for file in tqdm(files):\n",
        "        id = int(file.split('.jpg')[0])\n",
        "        if id in ids:\n",
        "            dic_ids.append(id)\n",
        "df_train = df_train.set_index(keys='id').loc[dic_ids,]\n",
        "trainset = df_train.reset_index().iloc[:int(len(df_train) * 0.9)].drop(columns=['coarse_i','medium_i', 'fine_i'])\n",
        "valset = df_train.reset_index().iloc[int(len(df_train) * 0.9):].drop(columns=['coarse_i','medium_i', 'fine_i'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ie0odCGKo64q"
      },
      "outputs": [],
      "source": [
        "|import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVImageDataset(Dataset):\n",
        "    def __init__(self, annotations_df, img_dir, transform=None):\n",
        "        self.img_labels = annotations_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #todo: idx using image id?\n",
        "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0]) + '.jpg')\n",
        "        image = decode_image(img_path).float() / 255.0\n",
        "        label = torch.tensor([self.img_labels.iloc[idx, 1], self.img_labels.iloc[idx, 2]])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.clamp(0, 1)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SXHoflESo64q",
        "outputId": "ef016ec4-75c1-48ec-e625-9a16378493f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "KERNEL_SIZE = 16 #16x16 patch\n",
        "CHANNELS = 3 #rgb\n",
        "RESIZE = 224\n",
        "EMBED_DIM = CHANNELS * KERNEL_SIZE ** 2\n",
        "NUM_PATCHES = ((RESIZE + 0 - KERNEL_SIZE)//KERNEL_SIZE + 1) ** 2\n",
        "MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "#Using values the ViT was trained on\n",
        "processor = ViTImageProcessor.from_pretrained(MODEL_NAME, do_rescale = False, return_tensors = 'pt')\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "      v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "      v2.RandomHorizontalFlip(0.4),\n",
        "      v2.RandomVerticalFlip(0.1),\n",
        "      v2.RandomApply(transforms=[v2.RandomRotation(degrees=(0, 90))], p=0.5),\n",
        "      v2.RandomApply(transforms=[v2.ColorJitter(brightness=.3, hue=.1)], p=0.3),\n",
        "      v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9))], p=0.3),\n",
        "      normalize\n",
        " ])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVImageDataset(annotations_df = trainset, img_dir = INTERIM_DATA_DIR + 'train', transform=train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = OSVImageDataset(annotations_df = valset, img_dir = INTERIM_DATA_DIR + 'train', transform=test_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xuCEqWbso64q"
      },
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "\n",
        "class GeoLocator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocator, self).__init__()\n",
        "\n",
        "        self.backbone = ViTModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        self.layer1 = nn.Linear(self.backbone.config.hidden_size, self.backbone.config.hidden_size)\n",
        "        self.norm1 = nn.LayerNorm(self.backbone.config.hidden_size)\n",
        "        self.dropout1 = nn.Dropout(p=0.05)\n",
        "\n",
        "        self.layer2 = nn.Linear(self.backbone.config.hidden_size, 512)  # 512: embedding size of location encoder\n",
        "        self.norm2 = nn.LayerNorm(512)\n",
        "        self.dropout2 = nn.Dropout(p=0.05)\n",
        "\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract last hidden state from the ViT backbone\n",
        "        x1 = self.backbone(x).last_hidden_state\n",
        "        x1 = x1[:, 0, :]  # Use CLS token only\n",
        "\n",
        "        # First layer with normalization and dropout\n",
        "        a1 = func.leaky_relu(self.norm1(self.layer1(x1)))\n",
        "        a1 = self.dropout1(a1)\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        a2 = func.leaky_relu(self.norm2(self.layer2(a1)))\n",
        "        a2 = self.dropout2(a2)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.layer3(a2)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xS8SMry6o64q",
        "outputId": "76d4dba0-40af-4db0-b368-6cad353dca19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "5419fd44ffc2411eaab5e90a9f61c7f7",
            "a3920e3171424966873e15d0dd8a300b",
            "c28ceb5fdc884bd28b8f59e327d74b0c",
            "70233ebd956f48f3b0a124a26f41bed7",
            "62588e5658914a43a1b92fa330f182e6",
            "8b2b73a4c2024b4ba1cb14653fcabd71",
            "25e2a5ee0deb4a47adc6e5b6f1fd7451",
            "a67fbff786414348914962992e1027f9",
            "91d85b27712641a4a5562622ce89a269",
            "6b36a370d735442ebab2cec4162b41c7",
            "5079516a444a473b83bb3adbc4423deb",
            "973028a483fe462383397fa2695d9d5b",
            "ed0e8aaad0654269b49f95dafcea15da",
            "d3a0012a0d154cbc8c7748b30aa6af63",
            "312701148e73427aac2f87d841b6ba09",
            "fefca4dae13143799d17793fa31e71f9",
            "ad6c18b6ec9f4d92b61e66fbc355eef9",
            "8e6f923e95d8468299d21f1589662e7c",
            "28350702daf5453788aff680ce980df8",
            "18282704dd2e41f784df5e2e05879513",
            "03110a5b81d34113b8eab757283a2149",
            "f31b2935ea814afb841832d2f1fba7c7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/geoclip/model/location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5419fd44ffc2411eaab5e90a9f61c7f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973028a483fe462383397fa2695d9d5b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "model = GeoLocator().to(device=device)\n",
        "#freezing backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#optimizer for custom layers only\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.layer1.parameters()},\n",
        "    {'params': model.layer2.parameters()},\n",
        "    {'params': model.layer3.parameters()}\n",
        "], lr = 0.001)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CosineEmbeddingLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t6mFJ8VVo64q",
        "outputId": "42cb1b4d-0858-4e40-b0ff-008cdf373170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 28/29 [09:19<00:19, 19.98s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (1024) must match the size of tensor b (1014) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5501542d80bc>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#clearing memory so that my gpu doesn't die :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input1, input2, target)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m         return F.cosine_embedding_loss(\n\u001b[0m\u001b[1;32m   1424\u001b[0m             \u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcosine_embedding_loss\u001b[0;34m(input1, input2, target, margin, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3995\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_embedding_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (1014) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "scaler = GradScaler()\n",
        "num_epochs = 5\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for images, labels in tqdm(train_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = gps_encoder(labels.float().to(device=device))\n",
        "        with autocast(device_type=device.__str__()):\n",
        "            output = model(images)\n",
        "            ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "            loss = criterion(output, labels, ones)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del images, labels, ones\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjQpkSTco64r"
      },
      "outputs": [],
      "source": [
        "nn_path = \"/content/drive/MyDrive/Colab/where-am-i/geonn_v1.pt\"\n",
        "torch.save(model.state_dict(), str(nn_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1O2UWpfo64r"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        coarse_output, _, fine_output = model(images)\n",
        "\n",
        "        coarse_true = np.concatenate((coarse_true, labels[:,0].cpu()), axis=0)\n",
        "        coarse_pred = np.concatenate((coarse_pred, coarse_output.cpu()), axis=0)\n",
        "        fine_true = np.concatenate((fine_true, labels[:, 2].cpu()), axis=0)\n",
        "        fine_pred = np.concatenate((fine_pred, fine_output.cpu()), axis=0)\n",
        "\n",
        "        del images, labels, coarse_output, fine_output\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_w92c-Fo64r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "print(f'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}')\n",
        "print(f'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhhj31E2o64r"
      },
      "source": [
        "## Single Batch Perf check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gps_encoder(torch.tensor(trainset.loc[trainset['id'].isin([5467321436672143, 768748774029238]), ['latitude', 'longitude']].values).float().to(device=device))"
      ],
      "metadata": {
        "id": "T2jqFwUi3eX7",
        "outputId": "df69ca27-5905-45b6-9ed2-b1b8c6ea8c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0143, -0.0076,  0.0184,  ..., -0.0062,  0.0196,  0.0093],\n",
              "        [ 0.0128,  0.0089,  0.0040,  ..., -0.0172,  0.0044, -0.0020]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = trainset['id'].apply(lambda x : '/content/train/' + str(x) + '.jpg').tolist()"
      ],
      "metadata": {
        "id": "WhPep_vXy-Q7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nvidia.dali.pipeline import pipeline_def\n",
        "import nvidia.dali.types as types\n",
        "import nvidia.dali.fn as fn\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
        "import os\n",
        "\n",
        "@pipeline_def(num_threads=4, device_id=0)\n",
        "def get_dali_pipeline():\n",
        "    images, labels = fn.readers.file(\n",
        "        files=img_dir, random_shuffle=True, name=\"Reader\")\n",
        "    # decode data on the GPU\n",
        "    images = fn.decoders.image_random_crop(\n",
        "        images, device=\"mixed\", output_type=types.RGB)\n",
        "    # the rest of processing happens on the GPU as well\n",
        "    images = fn.resize(images, resize_x=256, resize_y=256)\n",
        "    images = fn.crop_mirror_normalize(\n",
        "        images,\n",
        "        crop_h=224,\n",
        "        crop_w=224,\n",
        "        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
        "        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n",
        "        mirror=fn.random.coin_flip())\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "train_data = DALIGenericIterator(\n",
        "    [get_dali_pipeline(batch_size=BATCH_SIZE)],\n",
        "    ['image', 'index'],\n",
        "    reader_name='Reader'\n",
        ")"
      ],
      "metadata": {
        "id": "5v9XGFTsvl51"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for i, data in enumerate(train_data):\n",
        "    c0 = time.perf_counter()\n",
        "    images = data[0]['image'].to(device=device)\n",
        "    c1 = time.perf_counter()\n",
        "    indices = data[0]['index'].numpy().flatten().tolist()\n",
        "    labels = torch.tensor(trainset.loc[indices, ['latitude', 'longitude']].values)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c2 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "    c3 = time.perf_counter()\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "train_data.reset()\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c0 - start}')\n",
        "print(f'img to gpu time for {BATCH_SIZE}: {c1 - c1}')\n",
        "print(f'embed time for {BATCH_SIZE}: {c2 - c1}')\n",
        "print(f'model time for {BATCH_SIZE}: {c3 - c2}')"
      ],
      "metadata": {
        "id": "hkaZgsWFwgP9",
        "outputId": "9fbf504c-989c-41a7-f6d9-a07d244d0781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-9e5df52f9e42>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pYhtYKgW_-gi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SHdGaV7go64s",
        "outputId": "4e02918d-2c34-4933-8f3f-18402aef33c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time for 2048: 14.727381185000013\n",
            "transform time for 2048: 13.182958095999993\n",
            "img to gpu time for 2048: 0.0\n",
            "embed time for 2048: 0.004340554999998858\n",
            "model time for 2048: 0.6053318549998039\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    c0 = time.perf_counter()\n",
        "    images = images.to(device=device)\n",
        "    c1 = time.perf_counter()\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c2 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "    c3 = time.perf_counter()\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c0 - start}')\n",
        "print(f'img to gpu time for {BATCH_SIZE}: {c1 - c1}')\n",
        "print(f'embed time for {BATCH_SIZE}: {c2 - c1}')\n",
        "print(f'model time for {BATCH_SIZE}: {c3 - c2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKZleKwJo64t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    images = images.to(device=device)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c1 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c1 - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJf2EaCwo64t"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.rename(columns={'coarse_i':'coarse', 'medium_i':'medium', 'fine_i':'fine'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46ZQZTHfo64t"
      },
      "outputs": [],
      "source": [
        "df = df_train.iloc[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDfs3uuno64t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_top_k_similar_embeddings(query_embedding, candidate_embeddings, k):\n",
        "    \"\"\"\n",
        "    Find the top k embeddings from the candidates based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (torch.Tensor): The embedding to compare against, shape (1, 512).\n",
        "        candidate_embeddings (torch.Tensor): Candidate embeddings, shape (N, 512).\n",
        "        k (int): Number of top embeddings to return.\n",
        "\n",
        "    Returns:\n",
        "        top_k_indices (list): Indices of the top k most similar embeddings.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), candidate_embeddings.cpu().numpy())[0]\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Sort in descending order\n",
        "    return top_k_indices\n",
        "\n",
        "def predict_locations(images, labels, dataloader, GeoLocator, gps_encoder, dataframe, k=5):\n",
        "    \"\"\"\n",
        "    Predict top k latitude and longitude/geohashes for given images.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images.\n",
        "        labels (torch.Tensor): Ground truth latitudes and longitudes.\n",
        "        dataloader (DataLoader): Dataloader for images and labels.\n",
        "        GeoLocator (torch.nn.Module): Trained model for predicting location embeddings.\n",
        "        gps_encoder (function): Function that encodes (lat, lon) into a 512D vector.\n",
        "        dataframe (pd.DataFrame): Dataframe with columns ['id', 'latitude', 'longitude', 'coarse', 'medium', 'fine'].\n",
        "        k (int): Number of top similar locations to return.\n",
        "\n",
        "    Returns:\n",
        "        results (list): A list of dictionaries with final top k predictions.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Precompute embeddings for unique coarse, medium, and fine geohashes\n",
        "    unique_coarse = dataframe.drop_duplicates(subset=['coarse'])\n",
        "    unique_medium = dataframe.drop_duplicates(subset=['medium'])\n",
        "    unique_fine = dataframe.drop_duplicates(subset=['fine'])\n",
        "\n",
        "    coarse_coords = torch.tensor(unique_coarse[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    medium_coords = torch.tensor(unique_medium[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    fine_coords = torch.tensor(unique_fine[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "\n",
        "    unique_coarse['embedding'] = gps_encoder(coarse_coords)\n",
        "    unique_medium['embedding'] = gps_encoder(medium_coords)\n",
        "    unique_fine['embedding'] = gps_encoder(fine_coords)\n",
        "\n",
        "    coarse_embeddings = torch.stack(unique_coarse['embedding'].tolist())\n",
        "    medium_embeddings = torch.stack(unique_medium['embedding'].tolist())\n",
        "    fine_embeddings = torch.stack(unique_fine['embedding'].tolist())\n",
        "\n",
        "    for batch_images, batch_labels in dataloader:\n",
        "        batch_results = []\n",
        "\n",
        "        # Predict embeddings using GeoLocator\n",
        "        predicted_embeddings = GeoLocator(batch_images)\n",
        "\n",
        "        for pred_embedding in predicted_embeddings:\n",
        "            # Step 1: Find top k coarse geohashes\n",
        "            top_k_coarse_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), coarse_embeddings, k)\n",
        "            top_k_coarse_candidates = unique_coarse.iloc[top_k_coarse_indices]\n",
        "\n",
        "            # Step 2: Find top k medium geohashes\n",
        "            medium_subset = unique_medium[unique_medium['coarse'].isin(top_k_coarse_candidates['coarse'])]\n",
        "            medium_embeddings_subset = torch.stack(medium_subset['embedding'].tolist())\n",
        "            top_k_medium_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), medium_embeddings_subset, k)\n",
        "            top_k_medium_candidates = medium_subset.iloc[top_k_medium_indices]\n",
        "\n",
        "            # Step 3: Find top k fine geohashes\n",
        "            fine_subset = unique_fine[unique_fine['medium'].isin(top_k_medium_candidates['medium'])]\n",
        "            fine_embeddings_subset = torch.stack(fine_subset['embedding'].tolist())\n",
        "            top_k_fine_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), fine_embeddings_subset, k)\n",
        "            final_top_k = fine_subset.iloc[top_k_fine_indices]\n",
        "\n",
        "            # Retrieve final top k latitudes and longitudes\n",
        "            batch_results.append(final_top_k[['latitude', 'longitude', 'fine']])\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1pWInOo64t"
      },
      "source": [
        "## Embedding to GPS Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNMBKuVho64t"
      },
      "outputs": [],
      "source": [
        "df = df_train.drop(columns=['id', 'coarse_i', 'medium_i', 'fine_i'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrGA0GB7o64t",
        "outputId": "bf4fad41-75bd-47ba-e423-b5d0b8c0c631"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "coords = torch.tensor(df[[\"latitude\", \"longitude\"]].values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = gps_encoder(coords).cpu().numpy()\n",
        "\n",
        "embedding_df = pd.DataFrame(\n",
        "    embeddings,\n",
        "    columns=[f\"embedding_{i+1}\" for i in range(embeddings.shape[1])]\n",
        ")\n",
        "\n",
        "# Concatenate the original DataFrame with the embeddings DataFrame\n",
        "df = pd.concat([df, embedding_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3Pj5n7o64u"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(INTERIM_DATA_DIR / 'embeddings.parquet').drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dyEbo_xo64u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingToGPSDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingToGPSDecoder, self).__init__()\n",
        "\n",
        "        # First layer: 512 (embedding size) -> 512\n",
        "        self.layer1 = nn.Linear(512, 512)\n",
        "        self.norm1 = nn.LayerNorm(512)\n",
        "\n",
        "        # Second layer: 512 -> 256\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.norm2 = nn.LayerNorm(256)\n",
        "\n",
        "        # Output layer: 256 -> 2 (latitude and longitude)\n",
        "        self.output_layer = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with normalization and dropout\n",
        "        x1 = F.leaky_relu(self.norm1(self.layer1(x)))\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        x2 = F.leaky_relu(self.norm2(self.layer2(x1)))\n",
        "\n",
        "        # Output layer for predicting latitude and longitude\n",
        "        gps_coordinates = self.output_layer(x2)\n",
        "\n",
        "        return gps_coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1yrEHi1o64u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, annotations_df):\n",
        "        self.df = annotations_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
        "        label = torch.tensor([self.df.iloc[idx, 0], self.df.iloc[idx, 1]])\n",
        "        return embedding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfopEQNAo64u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GeodesicDistanceLoss(nn.Module):\n",
        "    def __init__(self, radius=6371):\n",
        "        super(GeodesicDistanceLoss, self).__init__()\n",
        "        self.radius = radius  # Earth's radius in kilometers\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Convert latitude and longitude from degrees to radians\n",
        "        pred_rad = torch.deg2rad(pred)\n",
        "        target_rad = torch.deg2rad(target)\n",
        "\n",
        "        # Split latitudes and longitudes\n",
        "        lat1, lon1 = pred_rad[:, 0], pred_rad[:, 1]\n",
        "        lat2, lon2 = target_rad[:, 0], target_rad[:, 1]\n",
        "\n",
        "        # Haversine formula\n",
        "        delta_lat = lat2 - lat1\n",
        "        delta_lon = lon2 - lon1\n",
        "        a = torch.sin(delta_lat / 2) ** 2 + \\\n",
        "            torch.cos(lat1) * torch.cos(lat2) * torch.sin(delta_lon / 2) ** 2\n",
        "        c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "\n",
        "        # Distance in kilometers\n",
        "        distance = self.radius * c\n",
        "        return distance.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vghXaJpMo64u",
        "outputId": "ca1b3ffc-e007-4ce8-e574-5528610f9866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVEmbeddingsDataset(annotations_df = df)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyg3144Uo64u"
      },
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model = model.to(device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "criterion = GeodesicDistanceLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for embeddings, labels in tqdm(train_dataloader):\n",
        "        embeddings = embeddings.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "\n",
        "        output = model(embeddings)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del output, embeddings, labels\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step(loss)\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tg9pxlpo64v"
      },
      "outputs": [],
      "source": [
        "from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"reversenn.pt\"\n",
        "torch.save(model.state_dict(), MODELS_DIR / nn_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfBdfp7Eo64v"
      },
      "outputs": [],
      "source": [
        "#df.to_parquet(INTERIM_DATA_DIR / 'embeddings.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvLlXDQ4o64v",
        "outputId": "46003960-1ed0-45cf-a1ae-48c460ea7077"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.loc[:,['id','latitude','longitude']].equals(pd.concat([df_train['id'], df], axis=1).loc[:, ['id','latitude','longitude']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7leWfCXKo64v"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train['id'], df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_47iYE_po64v"
      },
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model.eval()\n",
        "gps_encoder = LocationEncoder()\n",
        "\n",
        "embeds = gps_encoder(torch.tensor([[41.659354389516, -111.86601253359]]))\n",
        "output = model(embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAPbS10po64v",
        "outputId": "99c4a107-1180-44fb-ad54-6345ff6b6fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  41.3965, -111.4439]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnlFj_rEo64v",
        "outputId": "1ab76374-9c6f-42d5-a46b-f6ab842fbedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45.75356838199693\n"
          ]
        }
      ],
      "source": [
        "from geopy import distance\n",
        "\n",
        "og = (41.659354389516, -111.86601253359)\n",
        "decoded = (41.3965, -111.4439)\n",
        "\n",
        "print(distance.distance(og, decoded).km)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXwLMUA6o64z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5419fd44ffc2411eaab5e90a9f61c7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3920e3171424966873e15d0dd8a300b",
              "IPY_MODEL_c28ceb5fdc884bd28b8f59e327d74b0c",
              "IPY_MODEL_70233ebd956f48f3b0a124a26f41bed7"
            ],
            "layout": "IPY_MODEL_62588e5658914a43a1b92fa330f182e6"
          }
        },
        "a3920e3171424966873e15d0dd8a300b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b2b73a4c2024b4ba1cb14653fcabd71",
            "placeholder": "​",
            "style": "IPY_MODEL_25e2a5ee0deb4a47adc6e5b6f1fd7451",
            "value": "config.json: 100%"
          }
        },
        "c28ceb5fdc884bd28b8f59e327d74b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a67fbff786414348914962992e1027f9",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91d85b27712641a4a5562622ce89a269",
            "value": 502
          }
        },
        "70233ebd956f48f3b0a124a26f41bed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b36a370d735442ebab2cec4162b41c7",
            "placeholder": "​",
            "style": "IPY_MODEL_5079516a444a473b83bb3adbc4423deb",
            "value": " 502/502 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "62588e5658914a43a1b92fa330f182e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2b73a4c2024b4ba1cb14653fcabd71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25e2a5ee0deb4a47adc6e5b6f1fd7451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a67fbff786414348914962992e1027f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d85b27712641a4a5562622ce89a269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b36a370d735442ebab2cec4162b41c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5079516a444a473b83bb3adbc4423deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "973028a483fe462383397fa2695d9d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed0e8aaad0654269b49f95dafcea15da",
              "IPY_MODEL_d3a0012a0d154cbc8c7748b30aa6af63",
              "IPY_MODEL_312701148e73427aac2f87d841b6ba09"
            ],
            "layout": "IPY_MODEL_fefca4dae13143799d17793fa31e71f9"
          }
        },
        "ed0e8aaad0654269b49f95dafcea15da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad6c18b6ec9f4d92b61e66fbc355eef9",
            "placeholder": "​",
            "style": "IPY_MODEL_8e6f923e95d8468299d21f1589662e7c",
            "value": "model.safetensors: 100%"
          }
        },
        "d3a0012a0d154cbc8c7748b30aa6af63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28350702daf5453788aff680ce980df8",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18282704dd2e41f784df5e2e05879513",
            "value": 345579424
          }
        },
        "312701148e73427aac2f87d841b6ba09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03110a5b81d34113b8eab757283a2149",
            "placeholder": "​",
            "style": "IPY_MODEL_f31b2935ea814afb841832d2f1fba7c7",
            "value": " 346M/346M [00:01&lt;00:00, 241MB/s]"
          }
        },
        "fefca4dae13143799d17793fa31e71f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad6c18b6ec9f4d92b61e66fbc355eef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e6f923e95d8468299d21f1589662e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28350702daf5453788aff680ce980df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18282704dd2e41f784df5e2e05879513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03110a5b81d34113b8eab757283a2149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f31b2935ea814afb841832d2f1fba7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}