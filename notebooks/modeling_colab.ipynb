{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install geoclip\n",
        "!pip install nvidia-dali-cuda120"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2bJsRpIb1_r6",
        "outputId": "ff121f18-788e-4186-b028-9b74713a28f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting geoclip\n",
            "  Downloading geoclip-1.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from geoclip) (0.20.1+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from geoclip) (11.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from geoclip) (4.47.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geoclip) (1.26.4)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (from geoclip) (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy->geoclip) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->geoclip) (2024.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->geoclip) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->geoclip) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->geoclip) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->geoclip) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->geoclip) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->geoclip) (2024.12.14)\n",
            "Downloading geoclip-1.2.0-py3-none-any.whl (40.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geoclip\n",
            "Successfully installed geoclip-1.2.0\n",
            "Collecting nvidia-dali-cuda120\n",
            "  Downloading nvidia_dali_cuda120-1.44.0.tar.gz (1.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: astunparse<=1.6.3,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.6.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.6.0)\n",
            "Collecting six<=1.16,>=1.16 (from nvidia-dali-cuda120)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: dm-tree<=0.1.8 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (0.1.8)\n",
            "Requirement already satisfied: packaging<=24.2 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda120) (24.2)\n",
            "Collecting nvidia-nvimgcodec-cu12<0.4.0,>=0.3.0 (from nvidia-dali-cuda120)\n",
            "  Downloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl.metadata (761 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse<=1.6.3,>=1.6.0->nvidia-dali-cuda120) (0.45.1)\n",
            "Downloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: nvidia-dali-cuda120\n",
            "  Building wheel for nvidia-dali-cuda120 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-dali-cuda120: filename=nvidia_dali_cuda120-1.44.0-20402542-py3-none-manylinux2014_x86_64.whl size=323567102 sha256=400b7b8db95a87702994a8f0475a29f463598a9a240de2c192423acdb0461909\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0c/89/4bc8a1da0bf7abe83d8759855f7cf5a3fa72c50a87eb89764d\n",
            "Successfully built nvidia-dali-cuda120\n",
            "Installing collected packages: six, nvidia-nvimgcodec-cu12, nvidia-dali-cuda120\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "Successfully installed nvidia-dali-cuda120-1.44.0 nvidia-nvimgcodec-cu12-0.3.0.5 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              },
              "id": "976d4d6959a241aca98a04bcedd8d926"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aoD10vio64o",
        "outputId": "e35807a8-37b8-479b-af15-54dd24428720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import geopy as gp\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "PkRC3vzno64p",
        "outputId": "9634cd7d-6c06-4a84-d452-b960eb3f6c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_1.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:34<08:03, 34.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_2.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [01:07<07:20, 33.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_3.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [01:48<07:22, 36.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_4.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [02:19<06:18, 34.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_5.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [02:54<05:48, 34.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_6.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [03:26<05:03, 33.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_7.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [03:55<04:17, 32.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_8.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [04:23<03:37, 31.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_9.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [04:52<03:02, 30.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_10.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [05:23<02:32, 30.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_11.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [05:55<02:03, 30.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_12.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [06:26<01:32, 30.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_13.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [07:01<01:04, 32.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_14.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [07:36<00:33, 33.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/drive/MyDrive/Colab/where-am-i/data/train_15.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [07:49<00:00, 31.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All train images extracted successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nzip_path = \"/content/drive/MyDrive/Colab/where-am-i/data/val/val.zip\"\\nextract_path = \"/content/val\"\\nif not os.path.exists(extract_path):\\n    with zipfile.ZipFile(zip_path, \\'r\\') as zip_ref:\\n        zip_ref.extractall(extract_path)\\nprint(\\'Val set \\')'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/Colab/where-am-i/data'  # Folder where chunks are stored\n",
        "EXTRACTION_FOLDER = '/content/train'  # Folder to extract images\n",
        "\n",
        "os.makedirs(EXTRACTION_FOLDER, exist_ok=True)\n",
        "\n",
        "# List chunks\n",
        "chunks = [f for f in os.listdir(DRIVE_FOLDER) if f.endswith('.zip')]\n",
        "#chunks = [f for f in os.listdir(DRIVE_FOLDER) if f == 'train_15.zip']\n",
        "\n",
        "# Extract all chunks\n",
        "for chunk in tqdm(chunks):\n",
        "    chunk_path = os.path.join(DRIVE_FOLDER, chunk)\n",
        "    print(f\"Extracting {chunk_path}...\")\n",
        "    shutil.unpack_archive(chunk_path, EXTRACTION_FOLDER)\n",
        "\n",
        "print(\"All train images extracted successfully!\")\n",
        "\n",
        "'''\n",
        "zip_path = \"/content/drive/MyDrive/Colab/where-am-i/data/val/val.zip\"\n",
        "extract_path = \"/content/val\"\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "print('Val set ')'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train = pd.read_parquet('/content/drive/MyDrive/Colab/where-am-i/embeddings.parquet')"
      ],
      "metadata": {
        "id": "Hiuk6DbOvxG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuYALNDjo64p",
        "outputId": "206710ef-b311-4754-876f-0c129cd20c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1206098/1206098 [00:01<00:00, 982160.55it/s]\n"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab/where-am-i/pos_train.csv')\n",
        "INTERIM_DATA_DIR = '/content/'\n",
        "\n",
        "ids = set(df_train.loc[:, 'id'].values.tolist())\n",
        "dic_ids = []\n",
        "for root, dirs, files in os.walk('/content/train'):\n",
        "    for file in tqdm(files):\n",
        "        id = int(file.split('.jpg')[0])\n",
        "        if id in ids:\n",
        "            dic_ids.append(id)\n",
        "df_train = df_train.set_index(keys='id').loc[dic_ids,]\n",
        "#trainset = df_train.reset_index().iloc[:int(len(df_train) * 0.9)].drop(columns=['coarse_i','medium_i', 'fine_i'])\n",
        "#valset = df_train.reset_index().iloc[int(len(df_train) * 0.9):].drop(columns=['coarse_i','medium_i', 'fine_i'])\n",
        "trainset = df_train.reset_index().drop(columns=['coarse_i','medium_i', 'fine_i'])\n",
        "img_dir = trainset['id'].apply(lambda x : '/content/train/' + str(x) + '.jpg').tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXHoflESo64q",
        "outputId": "5552b393-098e-4e0e-e1e7-f34aed15aad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from nvidia.dali.pipeline import pipeline_def\n",
        "import nvidia.dali.types as types\n",
        "import nvidia.dali.fn as fn\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator\n",
        "import os\n",
        "\n",
        "BATCH_SIZE = 896\n",
        "MODEL_NAME = 'microsoft/swin-base-patch4-window7-224-in22k'\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "@pipeline_def(num_threads=2, device_id=0)\n",
        "def DALIPreProcessorPipeline():\n",
        "    images, indices = fn.readers.file(files=img_dir, random_shuffle=True, name=\"Reader\")\n",
        "\n",
        "    # Decode and resize to 224x224\n",
        "    images = fn.decoders.image(images, device=\"mixed\", output_type=types.RGB)\n",
        "    images = fn.resize(images, resize_x=224, resize_y=224)\n",
        "\n",
        "    # Scale pixel values to [0, 1]\n",
        "    images = fn.crop_mirror_normalize(\n",
        "        images,\n",
        "        mean=[0.0, 0.0, 0.0],\n",
        "        std=[255.0, 255.0, 255.0],\n",
        "    )\n",
        "\n",
        "    # Horizontal and vertical flips\n",
        "    images = fn.flip(images, horizontal=fn.random.coin_flip(probability=0.2))\n",
        "    images = fn.flip(images, vertical=fn.random.coin_flip(probability=0.05))\n",
        "\n",
        "    images = fn.transpose(images, perm=[1, 2, 0])  # Convert from CHW to HWC\n",
        "\n",
        "    # Random rotation\n",
        "    rotation_angle = fn.random.uniform(range=[0, 90])\n",
        "    rotation_mask = fn.random.coin_flip(probability=0.1)\n",
        "    images = fn.rotate(images, angle=rotation_mask * rotation_angle)\n",
        "\n",
        "\n",
        "    # Color jitter (brightness adjustment)\n",
        "    brightness = fn.random.uniform(range=[1 - 0.2, 1 + 0.2])\n",
        "    color_jitter_mask = fn.random.coin_flip(probability=0.2)\n",
        "    images = fn.brightness_contrast(images, brightness=color_jitter_mask * brightness + (1 - color_jitter_mask))\n",
        "\n",
        "    # Gaussian blur\n",
        "    gaussian_blur_mask = fn.random.coin_flip(probability=0.05)\n",
        "    images = fn.gaussian_blur(images, window_size=gaussian_blur_mask * 5 + (1 - gaussian_blur_mask) * 1)\n",
        "\n",
        "    images = fn.transpose(images, perm=[2, 0, 1])  # Convert back to CHW\n",
        "    # Final normalization for ViT\n",
        "    images = fn.crop_mirror_normalize(\n",
        "        images,\n",
        "        crop_h=224,\n",
        "        crop_w=224,\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "\n",
        "    return images, indices\n",
        "\n",
        "pipeline = DALIPreProcessorPipeline(batch_size=BATCH_SIZE)\n",
        "train_dataloader = DALIGenericIterator(\n",
        "    [pipeline],\n",
        "    ['image', 'index'],\n",
        "    reader_name='Reader'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xuCEqWbso64q"
      },
      "outputs": [],
      "source": [
        "from transformers import SwinModel\n",
        "from PIL import Image\n",
        "\n",
        "class GeoLocator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocator, self).__init__()\n",
        "\n",
        "        self.backbone = SwinModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # Update the hidden size to match Swin Transformer's output\n",
        "        hidden_size = self.backbone.config.hidden_size #(batch, 49, 1024)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling to reduce [49, hidden_size] -> [1, hidden_size]\n",
        "\n",
        "        self.layer1 = nn.Linear(self.backbone.config.hidden_size, 768)\n",
        "        self.norm1 = nn.LayerNorm(768)\n",
        "        self.dropout1 = nn.Dropout(p=0.05)\n",
        "\n",
        "        self.layer2 = nn.Linear(768, 512)  # 512: embedding size of location encoder\n",
        "        self.norm2 = nn.LayerNorm(512)\n",
        "        self.dropout2 = nn.Dropout(p=0.05)\n",
        "\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract last hidden state from the Swin backbone\n",
        "        x1 = self.backbone(x).last_hidden_state # Shape: [batch_size, num_tokens, hidden_size]\n",
        "\n",
        "        # Apply global average pooling over the tokens\n",
        "        x1 = x1.permute(0, 2, 1)  # Change to [batch_size, hidden_size, num_tokens]\n",
        "        x1 = self.global_pool(x1)  # Shape: [batch_size, hidden_size, 1]\n",
        "        x1 = x1.squeeze(-1)       # Shape: [batch_size, hidden_size]\n",
        "\n",
        "        # First layer with normalization and dropout\n",
        "        a1 = func.leaky_relu(self.norm1(self.layer1(x1)))\n",
        "        a1 = self.dropout1(a1)\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        a2 = func.leaky_relu(self.norm2(self.layer2(a1)))\n",
        "        a2 = self.dropout2(a2)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.layer3(a2)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "1f2b8fcaf02f450dae9a961cba6fd6f6",
            "dc85262cea254a4aba0c01adcfc1312a",
            "2625b4205c484a6f8f74c4cc87017f79",
            "53f8a9d41926463bbd26f27b447f95e6",
            "e26c8c02543a48f99f5ff422a63223ff",
            "2844bbfceb7f4b14af1769a33b0fe7a1",
            "08689760789c44acbb1e58ccc26dcece",
            "e093d99fc80543598a2ac70095037ea1",
            "fad042a2e5f04bee81ed38b35b9dda10",
            "f9afdb3fee184ab48524f6cebefa55ea",
            "b023411a95b9443ea6180e679b52edd3",
            "e9536ba55f3f473bbbb41a397836d548",
            "bc4da53777114b00adce71052b2f66a6",
            "db140e2d7194420fa4af622161394ae8",
            "afdf6458f3d745928d000c7b416d107a",
            "fc3056df03f045d89a8c59bd596a8511",
            "8920755b21c94f0b957b8bbc9e085935",
            "8caea90ad5614572abbad886f010937d",
            "56413a1e436c42efb0c37da3d7e0231e",
            "a78411860ca548c885d5193c7c19b330",
            "502dd431c4694263871f836157338b56",
            "fe685ebb189440c9a7c4dc568d38e063"
          ]
        },
        "id": "xS8SMry6o64q",
        "outputId": "a3e908aa-26fe-4017-bfe6-522ace534e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/geoclip/model/location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f2b8fcaf02f450dae9a961cba6fd6f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/437M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9536ba55f3f473bbbb41a397836d548"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from geoclip import LocationEncoder\n",
        "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "model = GeoLocator().to(device=device)\n",
        "#freezing backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#optimizer for custom layers only\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': model.layer1.parameters()},\n",
        "    {'params': model.layer2.parameters()},\n",
        "    {'params': model.layer3.parameters()}\n",
        "], lr = 0.01)\n",
        "\n",
        "criterion = nn.CosineEmbeddingLoss()\n",
        "\n",
        "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_path = \"/content/drive/MyDrive/Colab/where-am-i/geoswin_v3.pt\"\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(nn_path, map_location=device)\n",
        "\n",
        "# Restore the state dicts\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
      ],
      "metadata": {
        "id": "SkZIcJYtMPvi",
        "outputId": "a82a093c-5cda-45dd-a4f9-d4b23f2c4b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-5173329cf004>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(nn_path, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6mFJ8VVo64q",
        "outputId": "b5ec842d-650f-4665-e6bf-80f690dc0e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1347/1347 [24:47<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Loss: 0.5722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1346/1347 [24:39<00:01,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/4], Loss: 0.5599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1346/1346 [24:41<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/4], Loss: 0.5440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1346/1346 [24:38<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/4], Loss: 0.5324\n"
          ]
        }
      ],
      "source": [
        "scaler = GradScaler()\n",
        "num_epochs =\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for i, data in enumerate(tqdm(train_dataloader)):\n",
        "        images = data[0]['image'].to(device=device)\n",
        "        indices = data[0]['index'].numpy().flatten().tolist()\n",
        "        labels = torch.tensor(trainset.loc[indices, ['latitude', 'longitude']].values)\n",
        "        labels = gps_encoder(labels.float().to(device=device))\n",
        "        with autocast(device_type=device.__str__()):\n",
        "            output = model(images)\n",
        "            ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "            loss = criterion(output, labels, ones)\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del images, labels, ones\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    #scheduler.step(loss)\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "        nn_path = f\"/content/drive/MyDrive/Colab/where-am-i/geoswim_v{4+epoch}.pt\"\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            #'scheduler_state_dict': scheduler.state_dict()\n",
        "        }, str(nn_path))\n",
        "        del loss\n",
        "        gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1O2UWpfo64r"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        coarse_output, _, fine_output = model(images)\n",
        "\n",
        "        coarse_true = np.concatenate((coarse_true, labels[:,0].cpu()), axis=0)\n",
        "        coarse_pred = np.concatenate((coarse_pred, coarse_output.cpu()), axis=0)\n",
        "        fine_true = np.concatenate((fine_true, labels[:, 2].cpu()), axis=0)\n",
        "        fine_pred = np.concatenate((fine_pred, fine_output.cpu()), axis=0)\n",
        "\n",
        "        del images, labels, coarse_output, fine_output\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_w92c-Fo64r"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "print(f'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}')\n",
        "print(f'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhhj31E2o64r"
      },
      "source": [
        "## Single Batch Perf check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for i, data in enumerate(train_dataloader):\n",
        "    c0 = time.perf_counter()\n",
        "    images = data[0]['image'].to(device=device)\n",
        "    c1 = time.perf_counter()\n",
        "    indices = data[0]['index'].numpy().flatten().tolist()\n",
        "    labels = torch.tensor(trainset.loc[indices, ['latitude', 'longitude']].values)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c2 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "    c3 = time.perf_counter()\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "train_dataloader.reset()\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c0 - start}')\n",
        "print(f'img to gpu time for {BATCH_SIZE}: {c1 - c1}')\n",
        "print(f'embed time for {BATCH_SIZE}: {c2 - c1}')\n",
        "print(f'model time for {BATCH_SIZE}: {c3 - c2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkaZgsWFwgP9",
        "outputId": "559b2547-2cf9-4365-a42e-8b5a2a030c9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n",
            "WARNING:root:DALI iterator does not support resetting while epoch is not finished.                              Ignoring...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time for 8: 0.8276858259998789\n",
            "transform time for 8: 0.007324507000021185\n",
            "img to gpu time for 8: 0.0\n",
            "embed time for 8: 0.010702840999783803\n",
            "model time for 8: 0.06922503400005553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model, gps_encoder\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "pYhtYKgW_-gi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHdGaV7go64s",
        "outputId": "4e02918d-2c34-4933-8f3f-18402aef33c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time for 2048: 14.727381185000013\n",
            "transform time for 2048: 13.182958095999993\n",
            "img to gpu time for 2048: 0.0\n",
            "embed time for 2048: 0.004340554999998858\n",
            "model time for 2048: 0.6053318549998039\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    c0 = time.perf_counter()\n",
        "    images = images.to(device=device)\n",
        "    c1 = time.perf_counter()\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c2 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(np.shape(labels)[0]).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "    c3 = time.perf_counter()\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c0 - start}')\n",
        "print(f'img to gpu time for {BATCH_SIZE}: {c1 - c1}')\n",
        "print(f'embed time for {BATCH_SIZE}: {c2 - c1}')\n",
        "print(f'model time for {BATCH_SIZE}: {c3 - c2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKZleKwJo64t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    images = images.to(device=device)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c1 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c1 - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJf2EaCwo64t"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.rename(columns={'coarse_i':'coarse', 'medium_i':'medium', 'fine_i':'fine'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46ZQZTHfo64t"
      },
      "outputs": [],
      "source": [
        "df = df_train.iloc[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDfs3uuno64t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_top_k_similar_embeddings(query_embedding, candidate_embeddings, k):\n",
        "    \"\"\"\n",
        "    Find the top k embeddings from the candidates based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        query_embedding (torch.Tensor): The embedding to compare against, shape (1, 512).\n",
        "        candidate_embeddings (torch.Tensor): Candidate embeddings, shape (N, 512).\n",
        "        k (int): Number of top embeddings to return.\n",
        "\n",
        "    Returns:\n",
        "        top_k_indices (list): Indices of the top k most similar embeddings.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), candidate_embeddings.cpu().numpy())[0]\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Sort in descending order\n",
        "    return top_k_indices\n",
        "\n",
        "def predict_locations(images, labels, dataloader, GeoLocator, gps_encoder, dataframe, k=5):\n",
        "    \"\"\"\n",
        "    Predict top k latitude and longitude/geohashes for given images.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images.\n",
        "        labels (torch.Tensor): Ground truth latitudes and longitudes.\n",
        "        dataloader (DataLoader): Dataloader for images and labels.\n",
        "        GeoLocator (torch.nn.Module): Trained model for predicting location embeddings.\n",
        "        gps_encoder (function): Function that encodes (lat, lon) into a 512D vector.\n",
        "        dataframe (pd.DataFrame): Dataframe with columns ['id', 'latitude', 'longitude', 'coarse', 'medium', 'fine'].\n",
        "        k (int): Number of top similar locations to return.\n",
        "\n",
        "    Returns:\n",
        "        results (list): A list of dictionaries with final top k predictions.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Precompute embeddings for unique coarse, medium, and fine geohashes\n",
        "    unique_coarse = dataframe.drop_duplicates(subset=['coarse'])\n",
        "    unique_medium = dataframe.drop_duplicates(subset=['medium'])\n",
        "    unique_fine = dataframe.drop_duplicates(subset=['fine'])\n",
        "\n",
        "    coarse_coords = torch.tensor(unique_coarse[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    medium_coords = torch.tensor(unique_medium[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    fine_coords = torch.tensor(unique_fine[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "\n",
        "    unique_coarse['embedding'] = gps_encoder(coarse_coords)\n",
        "    unique_medium['embedding'] = gps_encoder(medium_coords)\n",
        "    unique_fine['embedding'] = gps_encoder(fine_coords)\n",
        "\n",
        "    coarse_embeddings = torch.stack(unique_coarse['embedding'].tolist())\n",
        "    medium_embeddings = torch.stack(unique_medium['embedding'].tolist())\n",
        "    fine_embeddings = torch.stack(unique_fine['embedding'].tolist())\n",
        "\n",
        "    for batch_images, batch_labels in dataloader:\n",
        "        batch_results = []\n",
        "\n",
        "        # Predict embeddings using GeoLocator\n",
        "        predicted_embeddings = GeoLocator(batch_images)\n",
        "\n",
        "        for pred_embedding in predicted_embeddings:\n",
        "            # Step 1: Find top k coarse geohashes\n",
        "            top_k_coarse_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), coarse_embeddings, k)\n",
        "            top_k_coarse_candidates = unique_coarse.iloc[top_k_coarse_indices]\n",
        "\n",
        "            # Step 2: Find top k medium geohashes\n",
        "            medium_subset = unique_medium[unique_medium['coarse'].isin(top_k_coarse_candidates['coarse'])]\n",
        "            medium_embeddings_subset = torch.stack(medium_subset['embedding'].tolist())\n",
        "            top_k_medium_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), medium_embeddings_subset, k)\n",
        "            top_k_medium_candidates = medium_subset.iloc[top_k_medium_indices]\n",
        "\n",
        "            # Step 3: Find top k fine geohashes\n",
        "            fine_subset = unique_fine[unique_fine['medium'].isin(top_k_medium_candidates['medium'])]\n",
        "            fine_embeddings_subset = torch.stack(fine_subset['embedding'].tolist())\n",
        "            top_k_fine_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), fine_embeddings_subset, k)\n",
        "            final_top_k = fine_subset.iloc[top_k_fine_indices]\n",
        "\n",
        "            # Retrieve final top k latitudes and longitudes\n",
        "            batch_results.append(final_top_k[['latitude', 'longitude', 'fine']])\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1pWInOo64t"
      },
      "source": [
        "## Embedding to GPS Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNMBKuVho64t"
      },
      "outputs": [],
      "source": [
        "df = df_train.drop(columns=['id', 'coarse_i', 'medium_i', 'fine_i'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrGA0GB7o64t",
        "outputId": "bf4fad41-75bd-47ba-e423-b5d0b8c0c631"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "coords = torch.tensor(df[[\"latitude\", \"longitude\"]].values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = gps_encoder(coords).cpu().numpy()\n",
        "\n",
        "embedding_df = pd.DataFrame(\n",
        "    embeddings,\n",
        "    columns=[f\"embedding_{i+1}\" for i in range(embeddings.shape[1])]\n",
        ")\n",
        "\n",
        "# Concatenate the original DataFrame with the embeddings DataFrame\n",
        "df = pd.concat([df, embedding_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs3Pj5n7o64u"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(INTERIM_DATA_DIR / 'embeddings.parquet').drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dyEbo_xo64u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingToGPSDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingToGPSDecoder, self).__init__()\n",
        "\n",
        "        # First layer: 512 (embedding size) -> 512\n",
        "        self.layer1 = nn.Linear(512, 512)\n",
        "        self.norm1 = nn.LayerNorm(512)\n",
        "\n",
        "        # Second layer: 512 -> 256\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.norm2 = nn.LayerNorm(256)\n",
        "\n",
        "        # Output layer: 256 -> 2 (latitude and longitude)\n",
        "        self.output_layer = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with normalization and dropout\n",
        "        x1 = F.leaky_relu(self.norm1(self.layer1(x)))\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        x2 = F.leaky_relu(self.norm2(self.layer2(x1)))\n",
        "\n",
        "        # Output layer for predicting latitude and longitude\n",
        "        gps_coordinates = self.output_layer(x2)\n",
        "\n",
        "        return gps_coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1yrEHi1o64u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, annotations_df):\n",
        "        self.df = annotations_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
        "        label = torch.tensor([self.df.iloc[idx, 0], self.df.iloc[idx, 1]])\n",
        "        return embedding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfopEQNAo64u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GeodesicDistanceLoss(nn.Module):\n",
        "    def __init__(self, radius=6371):\n",
        "        super(GeodesicDistanceLoss, self).__init__()\n",
        "        self.radius = radius  # Earth's radius in kilometers\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Convert latitude and longitude from degrees to radians\n",
        "        pred_rad = torch.deg2rad(pred)\n",
        "        target_rad = torch.deg2rad(target)\n",
        "\n",
        "        # Split latitudes and longitudes\n",
        "        lat1, lon1 = pred_rad[:, 0], pred_rad[:, 1]\n",
        "        lat2, lon2 = target_rad[:, 0], target_rad[:, 1]\n",
        "\n",
        "        # Haversine formula\n",
        "        delta_lat = lat2 - lat1\n",
        "        delta_lon = lon2 - lon1\n",
        "        a = torch.sin(delta_lat / 2) ** 2 + \\\n",
        "            torch.cos(lat1) * torch.cos(lat2) * torch.sin(delta_lon / 2) ** 2\n",
        "        c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "\n",
        "        # Distance in kilometers\n",
        "        distance = self.radius * c\n",
        "        return distance.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vghXaJpMo64u",
        "outputId": "ca1b3ffc-e007-4ce8-e574-5528610f9866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVEmbeddingsDataset(annotations_df = df)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyg3144Uo64u"
      },
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model = model.to(device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "criterion = GeodesicDistanceLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for embeddings, labels in tqdm(train_dataloader):\n",
        "        embeddings = embeddings.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "\n",
        "        output = model(embeddings)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del output, embeddings, labels\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step(loss)\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tg9pxlpo64v"
      },
      "outputs": [],
      "source": [
        "from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"reversenn.pt\"\n",
        "torch.save(model.state_dict(), MODELS_DIR / nn_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfBdfp7Eo64v"
      },
      "outputs": [],
      "source": [
        "#df.to_parquet(INTERIM_DATA_DIR / 'embeddings.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvLlXDQ4o64v",
        "outputId": "46003960-1ed0-45cf-a1ae-48c460ea7077"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.loc[:,['id','latitude','longitude']].equals(pd.concat([df_train['id'], df], axis=1).loc[:, ['id','latitude','longitude']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7leWfCXKo64v"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train['id'], df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_47iYE_po64v"
      },
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model.eval()\n",
        "gps_encoder = LocationEncoder()\n",
        "\n",
        "embeds = gps_encoder(torch.tensor([[41.659354389516, -111.86601253359]]))\n",
        "output = model(embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAPbS10po64v",
        "outputId": "99c4a107-1180-44fb-ad54-6345ff6b6fe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  41.3965, -111.4439]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnlFj_rEo64v",
        "outputId": "1ab76374-9c6f-42d5-a46b-f6ab842fbedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45.75356838199693\n"
          ]
        }
      ],
      "source": [
        "from geopy import distance\n",
        "\n",
        "og = (41.659354389516, -111.86601253359)\n",
        "decoded = (41.3965, -111.4439)\n",
        "\n",
        "print(distance.distance(og, decoded).km)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXwLMUA6o64z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f2b8fcaf02f450dae9a961cba6fd6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc85262cea254a4aba0c01adcfc1312a",
              "IPY_MODEL_2625b4205c484a6f8f74c4cc87017f79",
              "IPY_MODEL_53f8a9d41926463bbd26f27b447f95e6"
            ],
            "layout": "IPY_MODEL_e26c8c02543a48f99f5ff422a63223ff"
          }
        },
        "dc85262cea254a4aba0c01adcfc1312a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2844bbfceb7f4b14af1769a33b0fe7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_08689760789c44acbb1e58ccc26dcece",
            "value": "config.json: 100%"
          }
        },
        "2625b4205c484a6f8f74c4cc87017f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e093d99fc80543598a2ac70095037ea1",
            "max": 1666520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fad042a2e5f04bee81ed38b35b9dda10",
            "value": 1666520
          }
        },
        "53f8a9d41926463bbd26f27b447f95e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9afdb3fee184ab48524f6cebefa55ea",
            "placeholder": "​",
            "style": "IPY_MODEL_b023411a95b9443ea6180e679b52edd3",
            "value": " 1.67M/1.67M [00:00&lt;00:00, 8.73MB/s]"
          }
        },
        "e26c8c02543a48f99f5ff422a63223ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2844bbfceb7f4b14af1769a33b0fe7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08689760789c44acbb1e58ccc26dcece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e093d99fc80543598a2ac70095037ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad042a2e5f04bee81ed38b35b9dda10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9afdb3fee184ab48524f6cebefa55ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b023411a95b9443ea6180e679b52edd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9536ba55f3f473bbbb41a397836d548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc4da53777114b00adce71052b2f66a6",
              "IPY_MODEL_db140e2d7194420fa4af622161394ae8",
              "IPY_MODEL_afdf6458f3d745928d000c7b416d107a"
            ],
            "layout": "IPY_MODEL_fc3056df03f045d89a8c59bd596a8511"
          }
        },
        "bc4da53777114b00adce71052b2f66a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8920755b21c94f0b957b8bbc9e085935",
            "placeholder": "​",
            "style": "IPY_MODEL_8caea90ad5614572abbad886f010937d",
            "value": "model.safetensors: 100%"
          }
        },
        "db140e2d7194420fa4af622161394ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56413a1e436c42efb0c37da3d7e0231e",
            "max": 437039150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a78411860ca548c885d5193c7c19b330",
            "value": 437039150
          }
        },
        "afdf6458f3d745928d000c7b416d107a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_502dd431c4694263871f836157338b56",
            "placeholder": "​",
            "style": "IPY_MODEL_fe685ebb189440c9a7c4dc568d38e063",
            "value": " 437M/437M [00:02&lt;00:00, 225MB/s]"
          }
        },
        "fc3056df03f045d89a8c59bd596a8511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8920755b21c94f0b957b8bbc9e085935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8caea90ad5614572abbad886f010937d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56413a1e436c42efb0c37da3d7e0231e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78411860ca548c885d5193c7c19b330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "502dd431c4694263871f836157338b56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe685ebb189440c9a7c4dc568d38e063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}