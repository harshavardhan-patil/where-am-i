{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-12-20 09:15:17.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: G:\\Work\\DS\\where-am-i\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import geopy as gp\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/Colab/where-am-i/data'  # Folder where chunks are stored\n",
        "EXTRACTION_FOLDER = '/content/train'  # Folder to extract images\n",
        "\n",
        "os.makedirs(EXTRACTION_FOLDER, exist_ok=True)\n",
        "\n",
        "# List chunks\n",
        "chunks = [f for f in os.listdir(DRIVE_FOLDER) if f == 'train_15.zip']\n",
        "\n",
        "# Extract all chunks\n",
        "for chunk in chunks:\n",
        "    chunk_path = os.path.join(DRIVE_FOLDER, chunk)\n",
        "    print(f\"Extracting {chunk_path}...\")\n",
        "    shutil.unpack_archive(chunk_path, EXTRACTION_FOLDER)\n",
        "\n",
        "print(\"All images extracted successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab/where-am-i/train.csv')\n",
        "INTERIM_DATA_DIR = '/content/'\n",
        "\n",
        "ids = set(df_train.loc[:, 'id'].values.tolist())\n",
        "dic_ids = []\n",
        "for root, dirs, files in os.walk('/content/train'):\n",
        "    for file in tqdm(files):\n",
        "        id = int(file.split('.jpg')[0])\n",
        "        if id in ids:\n",
        "            dic_ids.append(id)\n",
        "df_train = df_train.set_index(keys='id').loc[dic_ids,]\n",
        "trainset = df_train.reset_index().iloc[:int(len(df_train) * 0.9)]\n",
        "valset = df_train.reset_index().iloc[int(len(df_train) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_train.drop(columns=['coarse', 'medium', 'fine']).to_csv(INTERIM_DATA_DIR / 'train/train.csv', index = False)\n",
        "df_train = pd.read_csv(INTERIM_DATA_DIR / 'train.csv')\n",
        "df_train = pd.merge(df_train, pd.read_parquet(INTERIM_DATA_DIR / 'hashed_annos.parquet').loc[:, ['id', 'latitude', 'longitude']], on='id')#.drop(columns=['coarse_i','medium_i','fine_i'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_train.to_csv(INTERIM_DATA_DIR / 'pos_train.csv', index=False)\n",
        "df_train = pd.read_csv(INTERIM_DATA_DIR / 'pos_train.csv')\n",
        "trainset = df_train.iloc[:int(len(df_train) * 0.9)]\n",
        "valset = df_train.iloc[int(len(df_train) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVImageDataset(Dataset):\n",
        "    def __init__(self, annotations_df, img_dir, transform=None):\n",
        "        self.img_labels = annotations_df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #todo: idx using image id?\n",
        "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0]) + '.jpg')\n",
        "        image = decode_image(img_path).float() / 255.0\n",
        "        label = torch.tensor([self.img_labels.iloc[idx, 1], self.img_labels.iloc[idx, 2]])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.clamp(0, 1)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "KERNEL_SIZE = 16 #16x16 patch\n",
        "CHANNELS = 3 #rgb\n",
        "RESIZE = 224\n",
        "EMBED_DIM = CHANNELS * KERNEL_SIZE ** 2\n",
        "NUM_PATCHES = ((RESIZE + 0 - KERNEL_SIZE)//KERNEL_SIZE + 1) ** 2\n",
        "MODEL_NAME = 'google/vit-base-patch16-224-in21k'\n",
        "\n",
        "#Using values the ViT was trained on\n",
        "processor = ViTImageProcessor.from_pretrained(MODEL_NAME, do_rescale = False, return_tensors = 'pt')\n",
        "\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "train_transform = v2.Compose([\n",
        "      v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "      #v2.RandomHorizontalFlip(0.4),\n",
        "      #v2.RandomVerticalFlip(0.1),\n",
        "      #v2.RandomApply(transforms=[v2.RandomRotation(degrees=(0, 90))], p=0.5),\n",
        "      #v2.RandomApply(transforms=[v2.ColorJitter(brightness=.3, hue=.1)], p=0.3),\n",
        "      #v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9))], p=0.3),\n",
        "      normalize\n",
        " ])\n",
        "\n",
        "test_transform = v2.Compose([\n",
        "    v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVImageDataset(annotations_df = trainset, img_dir=INTERIM_DATA_DIR / 'train', transform=train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = OSVImageDataset(annotations_df = valset, img_dir = INTERIM_DATA_DIR / 'train', transform=test_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "\n",
        "class GeoLocator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeoLocator, self).__init__()\n",
        "        \n",
        "        self.backbone = ViTModel.from_pretrained(MODEL_NAME)\n",
        "        \n",
        "        self.layer1 = nn.Linear(self.backbone.config.hidden_size, self.backbone.config.hidden_size)\n",
        "        self.norm1 = nn.LayerNorm(self.backbone.config.hidden_size)\n",
        "        self.dropout1 = nn.Dropout(p=0.05)  \n",
        "        \n",
        "        self.layer2 = nn.Linear(self.backbone.config.hidden_size, 512)  # 512: embedding size of location encoder\n",
        "        self.norm2 = nn.LayerNorm(512)\n",
        "        self.dropout2 = nn.Dropout(p=0.05)\n",
        "        \n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract last hidden state from the ViT backbone\n",
        "        x1 = self.backbone(x).last_hidden_state\n",
        "        x1 = x1[:, 0, :]  # Use CLS token only\n",
        "\n",
        "        # First layer with normalization and dropout\n",
        "        a1 = func.leaky_relu(self.norm1(self.layer1(x1)))\n",
        "        a1 = self.dropout1(a1)\n",
        "        \n",
        "        # Second layer with normalization and dropout\n",
        "        a2 = func.leaky_relu(self.norm2(self.layer2(a1)))\n",
        "        a2 = self.dropout2(a2)\n",
        "        \n",
        "        # Output layer\n",
        "        output = self.layer3(a2)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "\n",
        "model = GeoLocator().to(device=device)\n",
        "#freezing backbone\n",
        "for param in model.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "#optimizer for custom layers only\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.layer1.parameters()},\n",
        "    {'params': model.layer2.parameters()},\n",
        "    {'params': model.layer3.parameters()}\n",
        "], lr = 0.001)\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CosineEmbeddingLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model.train()\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(device=device)\n",
        "        labels = gps_encoder(labels.float().to(device=device))\n",
        "        with autocast(device_type=device.__str__()):\n",
        "            output = model(images)\n",
        "            ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "            loss = criterion(output, labels, ones)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del images, labels, ones\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"geonn_t.pt\"\n",
        "torch.save(model.state_dict(), str(nn_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(val_dataloader):\n",
        "        images = images.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        coarse_output, _, fine_output = model(images)\n",
        "\n",
        "        coarse_true = np.concatenate((coarse_true, labels[:,0].cpu()), axis=0)\n",
        "        coarse_pred = np.concatenate((coarse_pred, coarse_output.cpu()), axis=0)\n",
        "        fine_true = np.concatenate((fine_true, labels[:, 2].cpu()), axis=0)\n",
        "        fine_pred = np.concatenate((fine_pred, fine_output.cpu()), axis=0)\n",
        "        \n",
        "        del images, labels, coarse_output, fine_output\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "\n",
        "print(f'Top K Accuracy Fine: {top_k_accuracy_score(fine_true, fine_pred, k=5, labels=[i for i in range(FINE)]) * 100}')\n",
        "print(f'Top K Accuracy Output: {top_k_accuracy_score(coarse_true, coarse_pred, k=5, labels=[i for i in range(COARSE)]) * 100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "((1.2e6/64) * 4)/3600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "((1.2e6/64) * 2)/3600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "((1.2e6/1024) * 6)/3600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "((3.2e4/64) * 2)/60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Batch Perf check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total time for 64: 0.8369551999494433\n",
            "transform time for 64: 0.8253547999775037\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    images = images.to(device=device)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c1 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c1 - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from geoclip import LocationEncoder\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "scaler = GradScaler()\n",
        "num_epochs = 10\n",
        "\n",
        "start = time.perf_counter()\n",
        "c1 = 0\n",
        "model.train()\n",
        "temp = 0\n",
        "for images, labels in train_dataloader:\n",
        "    images = images.to(device=device)\n",
        "    labels = gps_encoder(labels.float().to(device=device))\n",
        "    c1 = time.perf_counter()\n",
        "    with autocast(device_type=device.__str__()):\n",
        "        output = model(images)\n",
        "        ones = torch.ones(BATCH_SIZE).to(device=device)\n",
        "        loss = criterion(output, labels, ones)\n",
        "\n",
        "    #clearing memory so that my gpu doesn't die :)\n",
        "    del images, labels, ones\n",
        "    gc.collect()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    \n",
        "    del loss\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    if temp == 0:\n",
        "      break\n",
        "end = time.perf_counter()\n",
        "print(f'total time for {BATCH_SIZE}: {end - start}')\n",
        "print(f'transform time for {BATCH_SIZE}: {c1 - start}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = df_train.rename(columns={'coarse_i':'coarse', 'medium_i':'medium', 'fine_i':'fine'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_train.iloc[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_top_k_similar_embeddings(query_embedding, candidate_embeddings, k):\n",
        "    \"\"\"\n",
        "    Find the top k embeddings from the candidates based on cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        query_embedding (torch.Tensor): The embedding to compare against, shape (1, 512).\n",
        "        candidate_embeddings (torch.Tensor): Candidate embeddings, shape (N, 512).\n",
        "        k (int): Number of top embeddings to return.\n",
        "\n",
        "    Returns:\n",
        "        top_k_indices (list): Indices of the top k most similar embeddings.\n",
        "    \"\"\"\n",
        "    similarities = cosine_similarity(query_embedding.cpu().numpy(), candidate_embeddings.cpu().numpy())[0]\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]  # Sort in descending order\n",
        "    return top_k_indices\n",
        "\n",
        "def predict_locations(images, labels, dataloader, GeoLocator, gps_encoder, dataframe, k=5):\n",
        "    \"\"\"\n",
        "    Predict top k latitude and longitude/geohashes for given images.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images.\n",
        "        labels (torch.Tensor): Ground truth latitudes and longitudes.\n",
        "        dataloader (DataLoader): Dataloader for images and labels.\n",
        "        GeoLocator (torch.nn.Module): Trained model for predicting location embeddings.\n",
        "        gps_encoder (function): Function that encodes (lat, lon) into a 512D vector.\n",
        "        dataframe (pd.DataFrame): Dataframe with columns ['id', 'latitude', 'longitude', 'coarse', 'medium', 'fine'].\n",
        "        k (int): Number of top similar locations to return.\n",
        "\n",
        "    Returns:\n",
        "        results (list): A list of dictionaries with final top k predictions.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Precompute embeddings for unique coarse, medium, and fine geohashes\n",
        "    unique_coarse = dataframe.drop_duplicates(subset=['coarse'])\n",
        "    unique_medium = dataframe.drop_duplicates(subset=['medium'])\n",
        "    unique_fine = dataframe.drop_duplicates(subset=['fine'])\n",
        "\n",
        "    coarse_coords = torch.tensor(unique_coarse[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    medium_coords = torch.tensor(unique_medium[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "    fine_coords = torch.tensor(unique_fine[['latitude', 'longitude']].values, dtype=torch.float32)\n",
        "\n",
        "    unique_coarse['embedding'] = gps_encoder(coarse_coords)\n",
        "    unique_medium['embedding'] = gps_encoder(medium_coords)\n",
        "    unique_fine['embedding'] = gps_encoder(fine_coords)\n",
        "\n",
        "    coarse_embeddings = torch.stack(unique_coarse['embedding'].tolist())\n",
        "    medium_embeddings = torch.stack(unique_medium['embedding'].tolist())\n",
        "    fine_embeddings = torch.stack(unique_fine['embedding'].tolist())\n",
        "\n",
        "    for batch_images, batch_labels in dataloader:\n",
        "        batch_results = []\n",
        "\n",
        "        # Predict embeddings using GeoLocator\n",
        "        predicted_embeddings = GeoLocator(batch_images)\n",
        "\n",
        "        for pred_embedding in predicted_embeddings:\n",
        "            # Step 1: Find top k coarse geohashes\n",
        "            top_k_coarse_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), coarse_embeddings, k)\n",
        "            top_k_coarse_candidates = unique_coarse.iloc[top_k_coarse_indices]\n",
        "\n",
        "            # Step 2: Find top k medium geohashes\n",
        "            medium_subset = unique_medium[unique_medium['coarse'].isin(top_k_coarse_candidates['coarse'])]\n",
        "            medium_embeddings_subset = torch.stack(medium_subset['embedding'].tolist())\n",
        "            top_k_medium_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), medium_embeddings_subset, k)\n",
        "            top_k_medium_candidates = medium_subset.iloc[top_k_medium_indices]\n",
        "\n",
        "            # Step 3: Find top k fine geohashes\n",
        "            fine_subset = unique_fine[unique_fine['medium'].isin(top_k_medium_candidates['medium'])]\n",
        "            fine_embeddings_subset = torch.stack(fine_subset['embedding'].tolist())\n",
        "            top_k_fine_indices = find_top_k_similar_embeddings(pred_embedding.unsqueeze(0), fine_embeddings_subset, k)\n",
        "            final_top_k = fine_subset.iloc[top_k_fine_indices]\n",
        "\n",
        "            # Retrieve final top k latitudes and longitudes\n",
        "            batch_results.append(final_top_k[['latitude', 'longitude', 'fine']])\n",
        "\n",
        "        results.extend(batch_results)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding to GPS Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_train.drop(columns=['id', 'coarse_i', 'medium_i', 'fine_i'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\geoclip\\model\\location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n"
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "\n",
        "gps_encoder = LocationEncoder().to(device=device)\n",
        "\n",
        "coords = torch.tensor(df[[\"latitude\", \"longitude\"]].values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = gps_encoder(coords).cpu().numpy()\n",
        "\n",
        "embedding_df = pd.DataFrame(\n",
        "    embeddings, \n",
        "    columns=[f\"embedding_{i+1}\" for i in range(embeddings.shape[1])]\n",
        ")\n",
        "\n",
        "# Concatenate the original DataFrame with the embeddings DataFrame\n",
        "df = pd.concat([df, embedding_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_parquet(INTERIM_DATA_DIR / 'embeddings.parquet').drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingToGPSDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EmbeddingToGPSDecoder, self).__init__()\n",
        "\n",
        "        # First layer: 512 (embedding size) -> 512\n",
        "        self.layer1 = nn.Linear(512, 512)\n",
        "        self.norm1 = nn.LayerNorm(512)\n",
        "\n",
        "        # Second layer: 512 -> 256\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.norm2 = nn.LayerNorm(256)\n",
        "\n",
        "        # Output layer: 256 -> 2 (latitude and longitude)\n",
        "        self.output_layer = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with normalization and dropout\n",
        "        x1 = F.leaky_relu(self.norm1(self.layer1(x)))\n",
        "\n",
        "        # Second layer with normalization and dropout\n",
        "        x2 = F.leaky_relu(self.norm2(self.layer2(x1)))\n",
        "\n",
        "        # Output layer for predicting latitude and longitude\n",
        "        gps_coordinates = self.output_layer(x2)\n",
        "\n",
        "        return gps_coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import decode_image, read_file\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "class OSVEmbeddingsDataset(Dataset):\n",
        "    def __init__(self, annotations_df):\n",
        "        self.df = annotations_df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
        "        label = torch.tensor([self.df.iloc[idx, 0], self.df.iloc[idx, 1]])\n",
        "        return embedding, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GeodesicDistanceLoss(nn.Module):\n",
        "    def __init__(self, radius=6371):\n",
        "        super(GeodesicDistanceLoss, self).__init__()\n",
        "        self.radius = radius  # Earth's radius in kilometers\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Convert latitude and longitude from degrees to radians\n",
        "        pred_rad = torch.deg2rad(pred)\n",
        "        target_rad = torch.deg2rad(target)\n",
        "\n",
        "        # Split latitudes and longitudes\n",
        "        lat1, lon1 = pred_rad[:, 0], pred_rad[:, 1]\n",
        "        lat2, lon2 = target_rad[:, 0], target_rad[:, 1]\n",
        "\n",
        "        # Haversine formula\n",
        "        delta_lat = lat2 - lat1\n",
        "        delta_lon = lon2 - lon1\n",
        "        a = torch.sin(delta_lat / 2) ** 2 + \\\n",
        "            torch.cos(lat1) * torch.cos(lat2) * torch.sin(delta_lon / 2) ** 2\n",
        "        c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
        "\n",
        "        # Distance in kilometers\n",
        "        distance = self.radius * c\n",
        "        return distance.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "from torchvision import datasets, transforms\n",
        "#from src.base.OSVImageDataset import OSVImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTImageProcessor\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_dataset = OSVEmbeddingsDataset(annotations_df = df)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1178 [00:00<?, ?it/s]C:\\Users\\Harshavardhan Patil\\AppData\\Local\\Temp\\ipykernel_19844\\4128849953.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  embedding = torch.tensor(self.df.iloc[idx, 2:], dtype=torch.float)\n",
            "100%|██████████| 1178/1178 [14:52<00:00,  1.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2024-12-20 09:55:58.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mEpoch [1/5], Loss: 26.3345\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 9/1178 [00:07<15:13,  1.28it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m embeddings, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     20\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     21\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[25], line 16\u001b[0m, in \u001b[0;36mOSVEmbeddingsDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 16\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     17\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding, label\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[1;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\indexing.py:1692\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1690\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m-> 1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[0;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[1;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\frame.py:3996\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3994\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[0;32m   3995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3996\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3998\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[0;32m   3999\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Harshavardhan Patil\\.virtualenvs\\where-am-i-s7vJJwrF\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1002\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, rl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(blk\u001b[38;5;241m.\u001b[39mmgr_locs):\n\u001b[1;32m-> 1002\u001b[0m         result[rl] \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mconstruct_array_type()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model = model.to(device=device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "criterion = GeodesicDistanceLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in (range(num_epochs)):\n",
        "    model.train()\n",
        "    for embeddings, labels in tqdm(train_dataloader):\n",
        "        embeddings = embeddings.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        \n",
        "        output = model(embeddings)\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        #clearing memory so that my gpu doesn't die :)\n",
        "        del output, embeddings, labels\n",
        "        gc.collect()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step(loss) \n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        del loss\n",
        "        gc.collect\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.config import MODELS_DIR\n",
        "\n",
        "nn_path = \"reversenn.pt\"\n",
        "torch.save(model.state_dict(), MODELS_DIR / nn_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_parquet(INTERIM_DATA_DIR / 'embeddings.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.loc[:,['id','latitude','longitude']].equals(pd.concat([df_train['id'], df], axis=1).loc[:, ['id','latitude','longitude']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([df_train['id'], df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from geoclip import LocationEncoder\n",
        "import gc\n",
        "from torch.amp import autocast, GradScaler\n",
        "from loguru import logger\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from src.config import MODELS_DIR\n",
        "\n",
        "model = EmbeddingToGPSDecoder()\n",
        "model.load_state_dict(torch.load(MODELS_DIR / \"reversenn.pt\", weights_only=True))\n",
        "model.eval()\n",
        "gps_encoder = LocationEncoder()\n",
        "\n",
        "embeds = gps_encoder(torch.tensor([[41.659354389516, -111.86601253359]]))\n",
        "output = model(embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  41.3965, -111.4439]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45.75356838199693\n"
          ]
        }
      ],
      "source": [
        "from geopy import distance\n",
        "\n",
        "og = (41.659354389516, -111.86601253359)\n",
        "decoded = (41.3965, -111.4439)\n",
        "\n",
        "print(distance.distance(og, decoded).km)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "where-am-i-s7vJJwrF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
